{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning du Modèle LLAMA 3\n",
    "\n",
    "Ce notebook a pour objectif d'effectuer un fine-tuning du modèle **LLAMA 3** en utilisant un dataset personnalisé.  \n",
    "Nous allons suivre les étapes suivantes :\n",
    "\n",
    "1. **Installation des Dépendances** : Installation des bibliothèques nécessaires, notamment `transformers`, `bitsandbytes`, `datasets` et `peft`.  \n",
    "2. **Connexion à Hugging Face et Téléchargement du Modèle** : Authentification à Hugging Face et chargement du modèle [`Llama-3.2-1B-Instruct`](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct).  \n",
    "3. **Traitement du Jeu de Données** : Prétraitement des données, tokenization et mise en forme pour le fine-tuning.  \n",
    "4. **Entraînement du Modèle** : Fine-tuning en utilisant **LoRA** et **QLoRA** pour optimiser la consommation mémoire.  \n",
    "5. **Sauvegarde du Modèle et Conversion vers Ollama** : Exportation du modèle entraîné au format compatible **Ollama** pour une utilisation simplifiée en local.  \n",
    "\n",
    "Nous utiliserons **`transformers`** de Hugging Face pour le modèle, **`datasets`** pour la gestion des données, et **`peft`** pour des optimisations d'entraînement sur GPU.\n",
    "\n",
    "💡 **Objectif** : Adapter LLAMA 3 à une tâche spécifique tout en optimisant les performances et la consommation mémoire.\n",
    "\n",
    "🚀 **Commençons !**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Installation des Dépendances\n",
    "\n",
    "Avant de commencer le fine-tuning de LLAMA 3, assurez-vous d’installer les logiciels recommandés :\n",
    "\n",
    "✅ **Git Bash** → [Télécharger ici](https://git-scm.com/downloads)  \n",
    "✅ **Python 3.12** → [Télécharger ici](https://www.python.org/downloads/release/python-3128/)  \n",
    "📌 *Pensez à cocher l’option \"Ajouter Python aux variables d’environnement\" lors de l’installation*  \n",
    "✅ **VS Code** → [Télécharger ici](https://code.visualstudio.com/Download)  \n",
    "✅ **Ollama** → [Télécharger ici](https://ollama.com/download)  \n",
    "\n",
    "---\n",
    "\n",
    "### 📌 Exécution du script d’installation\n",
    "\n",
    "Une fois les logiciels installés, ouvrez un terminal **Git Bash** et exécutez la commande suivante pour lancer l’installation des dépendances et du projet :\n",
    "\n",
    "   ```bash\n",
    "   bash installation.sh\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Connexion à Hugging Face\n",
    "\n",
    "Pour télécharger et utiliser le modèle **Llama-3.2-1B-Instruct**, vous devez d’abord configurer votre accès à Hugging Face.\n",
    "\n",
    "### 🔹 Étape 1 : Créer un compte Hugging Face\n",
    "Si vous n’avez pas encore de compte, créez-en un ici :  \n",
    "👉 [Créer un compte Hugging Face](https://huggingface.co/join)\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Étape 2 : Générer un Token d’authentification\n",
    "1. Rendez-vous dans les paramètres des tokens :  \n",
    "   👉 [Créer un token ici](https://huggingface.co/settings/tokens)\n",
    "2. Cliquez sur **New token**.\n",
    "3. Donnez-lui un nom (ex. : `llama3-token`) et attribuez-lui les droits **read**.\n",
    "4. Copiez le token généré.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Étape 3 : Demander l'accès au modèle LLAMA 3  \n",
    "Le modèle **Llama-3.2-1B-Instruct** nécessite une demande d'accès.  \n",
    "Rendez-vous sur cette page et cliquez sur **Request access** :  \n",
    "👉 [Demander l’accès au modèle](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct)  \n",
    "\n",
    "---\n",
    "\n",
    "### 🔹 Étape 4 : Connexion à Hugging Face dans le Notebook  \n",
    "\n",
    "Exécutez le code suivant pour vous connecter à Hugging Face avec votre token dans le terminal :\n",
    "   ```\n",
    "   huggingface-cli login\n",
    "   ```\n",
    "---\n",
    "\n",
    "### 🔹 Étape 5 : Téléchargement et Chargement du Modèle LLAMA 3  \n",
    "\n",
    "Une fois connecté, on peut télécharger et charger le modèle :\n",
    "      ```\n",
    "\n",
    "      from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "      MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "\n",
    "      ### Chargement du tokenizer et du modèle\n",
    "      tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "      model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\")\n",
    "\n",
    "      print(\"✅ Modèle LLAMA 3 chargé avec succès !\")\n",
    "\n",
    "      ```\n",
    "🚀 **Votre modèle LLAMA 3 est maintenant prêt à être utilisé !**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Quera\\Desktop\\INRAE-Llama3\\.llama_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Qui-es-tu ?\"\n",
    "\n",
    "# Tokeniser l'entrée\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Visualisation des tokens d'entrée\n",
    "outputs =model.generate(**inputs, max_length=40)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Visualisation des tokens de sorties\n",
    "outputs =model.generate(**inputs, max_length=40)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Qui-es-tu? (I am you)\\nI am you\\nI am you\\n\\nUn peu de philosophie\\n\\nUn être humain est une forme de vie complexe, avec des bes'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Décodage des tokens de sortie\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('llama3-1B\\\\tokenizer_config.json',\n",
       " 'llama3-1B\\\\special_tokens_map.json',\n",
       " 'llama3-1B\\\\tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sauvegarde du modèle \n",
    "model.save_pretrained(\"llama3-1B\")\n",
    "tokenizer.save_pretrained(\"llama3-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: llama3-1B\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {32}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.float32 --> F16, shape = {2048, 128256}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 131072\n",
      "INFO:hf-to-gguf:gguf: embedding length = 2048\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
      "INFO:hf-to-gguf:gguf: head count = 32\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 1\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 280147 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 128000\n",
      "INFO:gguf.vocab:Setting special token type eos to 128009\n",
      "INFO:gguf.vocab:Setting chat_template to {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- if strftime_now is defined %}\n",
      "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
      "    {%- else %}\n",
      "        {%- set date_string = \"26 Jul 2024\" %}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "        {{- '\"parameters\": ' }}\n",
      "        {{- tool_call.arguments | tojson }}\n",
      "        {{- \"}\" }}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:llama3-1B\\Llama-3.2-1B-Instruct-F16.gguf: n_tensors = 147, total_size = 2.5G\n",
      "\n",
      "Writing:   0%|          | 0.00/2.47G [00:00<?, ?byte/s]\n",
      "Writing:  21%|██▏       | 525M/2.47G [00:02<00:07, 261Mbyte/s]\n",
      "Writing:  23%|██▎       | 559M/2.47G [00:02<00:07, 252Mbyte/s]\n",
      "Writing:  24%|██▍       | 592M/2.47G [00:02<00:07, 248Mbyte/s]\n",
      "Writing:  25%|██▌       | 626M/2.47G [00:02<00:07, 242Mbyte/s]\n",
      "Writing:  28%|██▊       | 681M/2.47G [00:02<00:07, 239Mbyte/s]\n",
      "Writing:  29%|██▉       | 714M/2.47G [00:02<00:07, 235Mbyte/s]\n",
      "Writing:  30%|███       | 748M/2.47G [00:03<00:07, 234Mbyte/s]\n",
      "Writing:  32%|███▏      | 802M/2.47G [00:03<00:06, 239Mbyte/s]\n",
      "Writing:  34%|███▍      | 836M/2.47G [00:03<00:06, 238Mbyte/s]\n",
      "Writing:  35%|███▌      | 869M/2.47G [00:03<00:06, 243Mbyte/s]\n",
      "Writing:  37%|███▋      | 924M/2.47G [00:03<00:06, 257Mbyte/s]\n",
      "Writing:  39%|███▊      | 957M/2.47G [00:03<00:05, 264Mbyte/s]\n",
      "Writing:  40%|████      | 991M/2.47G [00:03<00:05, 264Mbyte/s]\n",
      "Writing:  42%|████▏     | 1.05G/2.47G [00:04<00:05, 271Mbyte/s]\n",
      "Writing:  44%|████▎     | 1.08G/2.47G [00:04<00:05, 264Mbyte/s]\n",
      "Writing:  45%|████▌     | 1.11G/2.47G [00:04<00:05, 254Mbyte/s]\n",
      "Writing:  47%|████▋     | 1.17G/2.47G [00:04<00:05, 247Mbyte/s]\n",
      "Writing:  49%|████▊     | 1.20G/2.47G [00:04<00:05, 245Mbyte/s]\n",
      "Writing:  50%|████▉     | 1.23G/2.47G [00:04<00:05, 245Mbyte/s]\n",
      "Writing:  52%|█████▏    | 1.29G/2.47G [00:05<00:04, 251Mbyte/s]\n",
      "Writing:  53%|█████▎    | 1.32G/2.47G [00:05<00:04, 233Mbyte/s]\n",
      "Writing:  55%|█████▍    | 1.36G/2.47G [00:05<00:04, 236Mbyte/s]\n",
      "Writing:  57%|█████▋    | 1.41G/2.47G [00:05<00:04, 234Mbyte/s]\n",
      "Writing:  58%|█████▊    | 1.44G/2.47G [00:05<00:04, 232Mbyte/s]\n",
      "Writing:  60%|█████▉    | 1.48G/2.47G [00:05<00:04, 237Mbyte/s]\n",
      "Writing:  62%|██████▏   | 1.53G/2.47G [00:06<00:03, 237Mbyte/s]\n",
      "Writing:  63%|██████▎   | 1.57G/2.47G [00:06<00:03, 240Mbyte/s]\n",
      "Writing:  65%|██████▍   | 1.60G/2.47G [00:06<00:03, 240Mbyte/s]\n",
      "Writing:  67%|██████▋   | 1.65G/2.47G [00:06<00:03, 219Mbyte/s]\n",
      "Writing:  68%|██████▊   | 1.69G/2.47G [00:06<00:03, 218Mbyte/s]\n",
      "Writing:  70%|██████▉   | 1.72G/2.47G [00:07<00:03, 215Mbyte/s]\n",
      "Writing:  72%|███████▏  | 1.78G/2.47G [00:07<00:03, 213Mbyte/s]\n",
      "Writing:  73%|███████▎  | 1.81G/2.47G [00:07<00:03, 210Mbyte/s]\n",
      "Writing:  75%|███████▍  | 1.84G/2.47G [00:07<00:03, 194Mbyte/s]\n",
      "Writing:  75%|███████▌  | 1.86G/2.47G [00:07<00:03, 176Mbyte/s]\n",
      "Writing:  77%|███████▋  | 1.90G/2.47G [00:08<00:03, 171Mbyte/s]\n",
      "Writing:  78%|███████▊  | 1.93G/2.47G [00:08<00:03, 173Mbyte/s]\n",
      "Writing:  79%|███████▉  | 1.96G/2.47G [00:08<00:03, 165Mbyte/s]\n",
      "Writing:  80%|████████  | 1.98G/2.47G [00:08<00:02, 166Mbyte/s]\n",
      "Writing:  82%|████████▏ | 2.02G/2.47G [00:08<00:02, 165Mbyte/s]\n",
      "Writing:  83%|████████▎ | 2.05G/2.47G [00:09<00:02, 169Mbyte/s]\n",
      "Writing:  84%|████████▍ | 2.09G/2.47G [00:09<00:02, 180Mbyte/s]\n",
      "Writing:  85%|████████▌ | 2.11G/2.47G [00:09<00:02, 180Mbyte/s]\n",
      "Writing:  87%|████████▋ | 2.14G/2.47G [00:09<00:01, 195Mbyte/s]\n",
      "Writing:  88%|████████▊ | 2.17G/2.47G [00:09<00:01, 205Mbyte/s]\n",
      "Writing:  89%|████████▉ | 2.21G/2.47G [00:09<00:01, 205Mbyte/s]\n",
      "Writing:  92%|█████████▏| 2.26G/2.47G [00:09<00:00, 223Mbyte/s]\n",
      "Writing:  93%|█████████▎| 2.30G/2.47G [00:10<00:00, 227Mbyte/s]\n",
      "Writing:  94%|█████████▍| 2.33G/2.47G [00:10<00:00, 222Mbyte/s]\n",
      "Writing:  96%|█████████▋| 2.38G/2.47G [00:10<00:00, 213Mbyte/s]\n",
      "Writing:  98%|█████████▊| 2.42G/2.47G [00:10<00:00, 215Mbyte/s]\n",
      "Writing:  99%|█████████▉| 2.45G/2.47G [00:10<00:00, 208Mbyte/s]\n",
      "Writing: 100%|██████████| 2.47G/2.47G [00:11<00:00, 209Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to llama3-1B\\Llama-3.2-1B-Instruct-F16.gguf\n"
     ]
    }
   ],
   "source": [
    "# Conversion en gguf pour utilisation dans Ollama\n",
    "\n",
    "!python ./llama.cpp/convert_hf_to_gguf.py ./llama3-1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Traitement du Jeu de Données\n",
    "\n",
    "## 📌 Format des données  \n",
    "\n",
    "Le modèle **LLAMA 3** attend des données sous un format spécifique.  \n",
    "Nous devons structurer les conversations en **JSON Lines** (`.jsonl`), où chaque ligne représente un échange entre l’utilisateur et l’assistant.\n",
    "\n",
    "Exemple de fichier `data.jsonl` :\n",
    "\n",
    "```json\n",
    "{\"messages\": [{\"role\": \"user\", \"content\": \"Qui es-tu ?\"}, {\"role\": \"assistant\", \"content\": \"Je suis l'assistant de l'INRAE<|endoftext|>\"}]}\n",
    "```\n",
    "\n",
    "### 🔹 Explication :  \n",
    "- **`role: user`** → Contenu du message de l’utilisateur  \n",
    "- **`role: assistant`** → Réponse générée par le modèle  \n",
    "- **`<|endoftext|>`** → **Token de fin de texte** qui marque la fin d’une conversation  \n",
    "\n",
    "Ce format est essentiel pour que le modèle apprenne à répondre correctement en conservant le contexte.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 Transformations appliquées  \n",
    "\n",
    "Le modèle **LLAMA 3** ne peut pas directement utiliser du texte brut. Nous devons **convertir** les messages en une forme que le modèle peut traiter :  \n",
    "- **`input_ids`** : Représentation des tokens du texte  \n",
    "- **`attention_mask`** : Masque indiquant quelles parties du texte doivent être prises en compte  \n",
    "- **`labels`** : Données de sortie attendues pour l’apprentissage  \n",
    "\n",
    "### 📍 **Avant transformation :**  \n",
    "Format brut des données :\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Qui es-tu ?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Je suis l'assistant de l'INRAE<|endoftext|>\"}\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 📍 **Après transformation :**  \n",
    "Format prêt pour le fine-tuning du modèle :\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Qui es-tu ?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Je suis l'assistant de l'INRAE<|endoftext|>\"}\n",
    "    ],\n",
    "    \"input_ids\": [128000, 62545, 1560, 2442, 84, 949, 30854, 36731, 326, 6, 78191, 409, 326, 6, 691, 5726, 36, 27, 91, 8862, 728, 428, 91, 29, 128009],\n",
    "    \"attention_mask\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
    "    \"labels\": [128000, 62545, 1560, 2442, 84, 949, 30854, 36731, 326, 6, 78191, 409, 326, 6, 691, 5726, 36, 27, 91, 8862, 728, 428, 91, 29, -100]\n",
    "}\n",
    "```\n",
    "\n",
    "### 🔹 **Explication des transformations :**  \n",
    "1. **Tokenization** :  \n",
    "   - Le texte est converti en **`input_ids`**, une séquence de nombres représentant chaque mot ou caractère.  \n",
    "   - Exemple : `\"Qui es-tu ?\"` → `[128000, 62545, 1560, 2442, 84]`  \n",
    "\n",
    "2. **Masking** (**`attention_mask`**) :  \n",
    "   - Indique quelles parties du texte doivent être prises en compte.  \n",
    "   - `1` = Token important, `0` = Token ignoré (padding).  \n",
    "\n",
    "3. **Labels pour l’entraînement** (**`labels`**) :  \n",
    "   - Même séquence que **`input_ids`**, sauf que les tokens d’entrée utilisateur sont remplacés par `-100` (pour ignorer leur contribution à la prédiction).  \n",
    "   - Cela permet au modèle de **prédire uniquement la réponse** et d’ignorer l’entrée utilisateur.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 4 examples [00:00, 137.62 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# ✅ Charger le dataset JSONL\n",
    "dataset = load_dataset('json', data_files='./data.jsonl')\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple de données : {'messages': [{'role': 'user', 'content': 'Qui es-tu ?'}, {'role': 'assistant', 'content': \"Je suis l'assistant de l'INRAE<|endoftext|>\"}]}\n"
     ]
    }
   ],
   "source": [
    "# ✅ Vérifier les clés du dataset\n",
    "print(\"Exemple de données :\", dataset['train'][0])  # Affiche un exemple pour vérifier sa structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du token de padding pour Lama3\n",
    "tokenizer.pad_token = tokenizer.eos_token  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4/4 [00:00<00:00, 172.96 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple de données : {'messages': [{'role': 'user', 'content': 'Qui es-tu ?'}, {'role': 'assistant', 'content': \"Je suis l'assistant de l'INRAE<|endoftext|>\"}], 'input_ids': [128000, 62545, 1560, 2442, 84, 949, 30854, 36731, 326, 6, 78191, 409, 326, 6, 691, 5726, 36, 27, 91, 8862, 728, 428, 91, 29, 128009], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], 'labels': [128000, 62545, 1560, 2442, 84, 949, 30854, 36731, 326, 6, 78191, 409, 326, 6, 691, 5726, 36, 27, 91, 8862, 728, 428, 91, 29, -100]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Prétraitement des données d'entrainement\n",
    "def preprocess_function(examples):\n",
    "    # 🔹 Construire le texte d'entrée à partir des messages\n",
    "    text_inputs = []\n",
    "    for message_set in examples[\"messages\"]:\n",
    "        text = \"\"\n",
    "        for message in message_set:\n",
    "            text += f\"{message['content']}\"\n",
    "        text_inputs.append(text.strip())\n",
    "\n",
    "    # 🔹 Tokenisation\n",
    "    inputs = tokenizer(text_inputs, truncation=True, padding=\"max_length\", max_length=25)\n",
    "\n",
    "    # 🔹 Copier input_ids pour labels\n",
    "    inputs[\"labels\"] = inputs[\"input_ids\"].copy()\n",
    "\n",
    "    # 🔹 Remplacer les tokens de padding par -100 pour ignorer la perte\n",
    "    padding_token_id = tokenizer.pad_token_id\n",
    "    inputs[\"labels\"] = [\n",
    "        [(label if label != padding_token_id else -100) for label in labels] for labels in inputs[\"labels\"]\n",
    "    ]\n",
    "\n",
    "    return inputs\n",
    "\n",
    "# ✅ Appliquer la transformation\n",
    "dataset = dataset.map(preprocess_function, batched=True)\n",
    "print(\"Exemple de données :\", dataset['train'][0])  # Affiche un exemple pour vérifier sa structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Entraînement du Modèle\n",
    "\n",
    "Après avoir préparé nos données, nous pouvons maintenant entraîner **LLAMA 3**.  \n",
    "Nous utiliserons **LoRA (Low-Rank Adaptation)** pour optimiser l'entraînement tout en **réduisant la charge mémoire**.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 4.1 Configuration du Fine-Tuning avec LoRA\n",
    "\n",
    "### ✅ **Pourquoi LoRA ?**\n",
    "Le fine-tuning complet d'un modèle **LLAMA 3** peut être très gourmand en ressources GPU.  \n",
    "LoRA permet de **réduire la quantité de poids entraînables** en n'entraînant que certaines couches du modèle.\n",
    "\n",
    "Nous définissons les paramètres de LoRA avec :\n",
    "\n",
    "```python\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# ✅ Config LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=32, lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "```\n",
    "\n",
    "### 📍 **Explication des paramètres :**\n",
    "- **`r=32`** → Taille du rang pour la factorisation des matrices. Plus il est élevé, plus l'entraînement est précis, mais coûteux.  \n",
    "- **`lora_alpha=32`** → Facteur de mise à l'échelle des poids LoRA.  \n",
    "- **`target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]`** → On applique LoRA uniquement sur certaines couches du modèle.  \n",
    "- **`lora_dropout=0.05`** → Ajout d'un **dropout** pour éviter l'overfitting.  \n",
    "- **`task_type=\"CAUSAL_LM\"`** → On entraîne un modèle **causal** (génération de texte).  \n",
    "\n",
    "Nous appliquons ensuite cette configuration au modèle :\n",
    "\n",
    "```python\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 4.2 Définition des Paramètres d'Entraînement\n",
    "\n",
    "Nous utilisons **`TrainingArguments`** pour définir les options d'entraînement :\n",
    "\n",
    "```python\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3-finetuned\",\n",
    "    per_device_train_batch_size=5,  # ⚠️ Réduction de la batch size car CPU limité\n",
    "    num_train_epochs=10,  # 🔹 10 époques pour un bon fine-tuning\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    learning_rate=2e-3,\n",
    "    gradient_accumulation_steps=90,  # 🔹 Augmenter pour réduire la charge mémoire\n",
    "    fp16=False,  # 🚫 Désactivé car inutile sur CPU\n",
    "    bf16=False,\n",
    "    gradient_checkpointing=False,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=100,\n",
    "    save_total_limit=2,  # 🔹 Garde seulement les 2 derniers checkpoints\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"tensorboard\",  # 🔹 Enregistre les logs pour TensorBoard\n",
    "    torch_compile=False,  # ✅ Optimisation CPU\n",
    "    no_cuda=True  # 🚫 Désactive l'utilisation du GPU\n",
    ")\n",
    "```\n",
    "\n",
    "### 📍 **Explication des paramètres :**\n",
    "- **`output_dir=\"./llama3-finetuned\"`** → Sauvegarde du modèle entraîné ici.  \n",
    "- **`per_device_train_batch_size=5`** → On utilise une petite batch size pour éviter d’épuiser la RAM.  \n",
    "- **`num_train_epochs=10`** → Nombre total de passes sur les données.  \n",
    "- **`gradient_accumulation_steps=90`** → Permet d’accumuler les gradients sur plusieurs batchs avant de mettre à jour les poids (réduit la charge mémoire).  \n",
    "- **`learning_rate=2e-3`** → Taux d’apprentissage initial.  \n",
    "- **`lr_scheduler_type=\"cosine\"`** → Programmation du taux d’apprentissage en fonction d’une courbe **cosinus**.  \n",
    "- **`save_total_limit=2`** → Évite d'enregistrer trop de checkpoints pour économiser de l'espace disque.  \n",
    "- **`torch_compile=False`** → Optimisation de l’entraînement pour CPU uniquement.  \n",
    "- **`no_cuda=True`** → **Forçage du CPU** (utile si aucun GPU disponible).  \n",
    "\n",
    "---\n",
    "\n",
    "### 📌 4.3 Préparation des Données pour l'Entraînement\n",
    "\n",
    "Nous utilisons **`DataCollatorForSeq2Seq`** pour normaliser les données avant l'entraînement.\n",
    "\n",
    "```python\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# ✅ Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\")\n",
    "```\n",
    "\n",
    "📍 **Pourquoi ?**  \n",
    "- Permet d'ajouter du **padding** pour que toutes les séquences aient la même longueur.  \n",
    "- Convertit les données en **tensors PyTorch** (`return_tensors=\"pt\"`).  \n",
    "\n",
    "---\n",
    "\n",
    "### 📌 4.4 Lancement de l’Entraînement  \n",
    "\n",
    "Enfin, nous utilisons **`Trainer`** pour entraîner le modèle avec nos données :\n",
    "\n",
    "```python\n",
    "from transformers import Trainer\n",
    "\n",
    "# ✅ Entraînement\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['train'],  # 🔹 On utilise le même dataset pour l’évaluation (peut être remplacé)\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "### 📍 **Explication :**\n",
    "- **`model=model`** → Modèle LoRA prêt à être entraîné.  \n",
    "- **`args=training_args`** → Paramètres d'entraînement définis plus tôt.  \n",
    "- **`train_dataset=dataset['train']`** → Données utilisées pour l'entraînement.  \n",
    "- **`eval_dataset=dataset['train']`** → Ici, on utilise le même dataset pour l’évaluation, mais idéalement il faut un dataset séparé.  \n",
    "- **`data_collator=data_collator`** → Regroupe les données de manière efficace.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------\n",
      "trainable params: 6,815,744 || all params: 1,242,630,144 || trainable%: 0.5485\n",
      "---------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Quera\\Desktop\\Llama3\\.llama\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "c:\\Users\\Quera\\Desktop\\Llama3\\.llama\\Lib\\site-packages\\transformers\\training_args.py:1609: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:57, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.148244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.027769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.843416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.643522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.435893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.198648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.951529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.705722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.452125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.106727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=2.3531587600708006, metrics={'train_runtime': 60.1425, 'train_samples_per_second': 0.665, 'train_steps_per_second': 0.166, 'total_flos': 7526107054080.0, 'train_loss': 2.3531587600708006, 'epoch': 10.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "# ✅ Config LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=32, lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "\n",
    "print(\"---------------------------------------------------------------\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "print(\"---------------------------------------------------------------\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3-finetuned\",\n",
    "    per_device_train_batch_size=5,  # ⚠️ Réduire la batch size car CPU est limité\n",
    "    num_train_epochs=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    learning_rate=2e-3,\n",
    "    gradient_accumulation_steps=90,  # 🔹 Augmenter pour réduire la charge mémoire\n",
    "    fp16=False,  # 🚫 Désactiver fp16 (inutile sur CPU)\n",
    "    bf16=False,\n",
    "    gradient_checkpointing=False,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=100,\n",
    "    save_total_limit=2,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"tensorboard\",\n",
    "    torch_compile=False,  # ✅ Optimisation CPU\n",
    "    no_cuda=True\n",
    ")\n",
    "\n",
    "\n",
    "# ✅ Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "# ✅ Entraînement\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['train'],\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:48, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.106727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.055463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.946546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.868594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.784750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.658026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.600254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.588399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.586216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.585886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=0.7959489822387695, metrics={'train_runtime': 51.1493, 'train_samples_per_second': 0.782, 'train_steps_per_second': 0.196, 'total_flos': 7526107054080.0, 'train_loss': 0.7959489822387695, 'epoch': 10.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deuxième série d'entrainement\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000,  62545,   1560,   2442,     84,    949]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformation des tokens d'entré\n",
    "prompt =\"Qui es-tu ?\"\n",
    "input_exemple = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Prédiction du modèle après entrainement\n",
    "output = model.generate(**input_exemple, max_length=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Qui es-tu?Je suis l'assistant de l'INRAE<|endoftext|>Je\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformation des tokens de sortie\n",
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Sauvegarde du Modèle et Conversion vers Ollama\n",
    "\n",
    "Une fois l’entraînement terminé, nous devons **enregistrer** notre modèle pour pouvoir l’utiliser ultérieurement.  \n",
    "\n",
    "Deux étapes sont nécessaires :\n",
    "1. **Sauvegarde du modèle entraîné avec LoRA**\n",
    "2. **Fusion des poids LoRA avec le modèle de base** pour obtenir un modèle autonome.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 5.1 Sauvegarde du Modèle LoRA\n",
    "\n",
    "Le modèle fine-tuné avec **LoRA** contient des poids adaptatifs séparés.  \n",
    "Nous devons sauvegarder :\n",
    "- **Le modèle LoRA**\n",
    "- **Le tokenizer** (indispensable pour la génération de texte)\n",
    "\n",
    "```python\n",
    "# Sauvegarde du modèle LoRA et du tokenizer\n",
    "model.save_pretrained(\"llama3-finetuned\")\n",
    "tokenizer.save_pretrained(\"llama3-finetuned\")\n",
    "```\n",
    "\n",
    "📍 **Explication :**  \n",
    "- `model.save_pretrained(\"llama3-finetuned\")` → Sauvegarde le modèle entraîné avec les poids LoRA.  \n",
    "- `tokenizer.save_pretrained(\"llama3-finetuned\")` → Sauvegarde le **tokenizer**, essentiel pour la conversion texte → tokens.  \n",
    "\n",
    "> 📝 **À noter** : À ce stade, le modèle dépend encore des poids **LoRA**.  \n",
    "> Il faut maintenant **fusionner ces poids** avec le modèle de base.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 5.2 Fusion des Poids LoRA avec le Modèle de Base\n",
    "\n",
    "LoRA ajoute uniquement des **ajustements** aux poids du modèle de base.  \n",
    "Si on veut un **modèle autonome**, il faut fusionner ces ajustements avec le modèle d’origine.\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# Chargement du modèle de base\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", device_map=\"cpu\")\n",
    "\n",
    "# Chargement du modèle LoRA et fusion avec le modèle de base\n",
    "peft_model = PeftModel.from_pretrained(base_model, \"llama3-finetuned\")\n",
    "peft_model = peft_model.merge_and_unload()\n",
    "```\n",
    "\n",
    "📍 **Explication :**  \n",
    "1. **On recharge le modèle de base** `meta-llama/Llama-3.2-1B-Instruct`  \n",
    "2. **On applique les poids LoRA** avec `PeftModel.from_pretrained(base_model, \"llama3-finetuned\")`  \n",
    "3. **On fusionne les poids** en appelant `merge_and_unload()`, ce qui intègre définitivement les ajustements LoRA dans le modèle principal.  \n",
    "\n",
    "Après cette fusion, le modèle devient totalement **indépendant de LoRA** et peut être utilisé comme un modèle standard.\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 3. Sauvegarde du Modèle Fusionné\n",
    "\n",
    "Une fois la fusion terminée, nous enregistrons **le modèle final** :\n",
    "\n",
    "```python\n",
    "# Sauvegarde du modèle fusionné (sans LoRA)\n",
    "peft_model.save_pretrained(\"llama3-finetuned-merged\")\n",
    "tokenizer.save_pretrained(\"llama3-finetuned-merged\")\n",
    "\n",
    "print(\"✅ Fusion et sauvegarde du modèle terminé !\")\n",
    "```\n",
    "\n",
    "📍 **Explication :**  \n",
    "- **`peft_model.save_pretrained(\"llama3-finetuned-merged\")`** → Sauvegarde le modèle final, prêt à être utilisé **sans dépendance LoRA**.  \n",
    "- **`tokenizer.save_pretrained(\"llama3-finetuned-merged\")`** → Sauvegarde le **tokenizer** avec le modèle fusionné.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fusion et sauvegarde du modèle terminé !\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarde du modèle LoRA et fusion des poids du modèle de base\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "model.save_pretrained(\"llama3-finetuned\")\n",
    "tokenizer.save_pretrained(\"llama3-finetuned\")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", device_map=\"cpu\")\n",
    "peft_model = PeftModel.from_pretrained(base_model, \"llama3-finetuned\")\n",
    "\n",
    "\n",
    "# Fusionner les poids LoRA avec le modèle principal\n",
    "peft_model = peft_model.merge_and_unload()\n",
    "\n",
    "# Sauvegarde du modèle fusionné (sans LoRA)\n",
    "peft_model.save_pretrained(\"llama3-finetuned-merged\")\n",
    "tokenizer.save_pretrained(\"llama3-finetuned-merged\")\n",
    "\n",
    "print(\"✅ Fusion et sauvegarde du modèle terminé !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: llama3-finetuned-merged\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {32}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.float32 --> F16, shape = {2048, 128256}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 131072\n",
      "INFO:hf-to-gguf:gguf: embedding length = 2048\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
      "INFO:hf-to-gguf:gguf: head count = 32\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 1\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 280147 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 128000\n",
      "INFO:gguf.vocab:Setting special token type eos to 128009\n",
      "INFO:gguf.vocab:Setting special token type pad to 128009\n",
      "INFO:gguf.vocab:Setting chat_template to {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- if strftime_now is defined %}\n",
      "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
      "    {%- else %}\n",
      "        {%- set date_string = \"26 Jul 2024\" %}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "        {{- '\"parameters\": ' }}\n",
      "        {{- tool_call.arguments | tojson }}\n",
      "        {{- \"}\" }}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:llama3-finetuned-merged\\Llama-3.2-1B-Instruct-F16.gguf: n_tensors = 147, total_size = 2.5G\n",
      "\n",
      "Writing:   0%|          | 0.00/2.47G [00:00<?, ?byte/s]\n",
      "Writing:  21%|██▏       | 525M/2.47G [00:03<00:13, 144Mbyte/s]\n",
      "Writing:  23%|██▎       | 559M/2.47G [00:03<00:13, 143Mbyte/s]\n",
      "Writing:  24%|██▍       | 592M/2.47G [00:04<00:12, 148Mbyte/s]\n",
      "Writing:  25%|██▌       | 626M/2.47G [00:04<00:12, 152Mbyte/s]\n",
      "Writing:  28%|██▊       | 681M/2.47G [00:04<00:11, 162Mbyte/s]\n",
      "Writing:  29%|██▉       | 714M/2.47G [00:04<00:10, 167Mbyte/s]\n",
      "Writing:  30%|███       | 748M/2.47G [00:04<00:10, 171Mbyte/s]\n",
      "Writing:  31%|███       | 769M/2.47G [00:04<00:09, 174Mbyte/s]\n",
      "Writing:  32%|███▏      | 802M/2.47G [00:05<00:09, 182Mbyte/s]\n",
      "Writing:  34%|███▍      | 836M/2.47G [00:05<00:08, 186Mbyte/s]\n",
      "Writing:  35%|███▌      | 869M/2.47G [00:05<00:08, 190Mbyte/s]\n",
      "Writing:  36%|███▌      | 890M/2.47G [00:05<00:08, 193Mbyte/s]\n",
      "Writing:  37%|███▋      | 924M/2.47G [00:05<00:08, 191Mbyte/s]\n",
      "Writing:  39%|███▊      | 957M/2.47G [00:05<00:07, 195Mbyte/s]\n",
      "Writing:  40%|████      | 991M/2.47G [00:06<00:07, 194Mbyte/s]\n",
      "Writing:  42%|████▏     | 1.05G/2.47G [00:06<00:07, 189Mbyte/s]\n",
      "Writing:  44%|████▎     | 1.08G/2.47G [00:06<00:07, 189Mbyte/s]\n",
      "Writing:  45%|████▌     | 1.11G/2.47G [00:06<00:07, 191Mbyte/s]\n",
      "Writing:  47%|████▋     | 1.17G/2.47G [00:07<00:06, 195Mbyte/s]\n",
      "Writing:  49%|████▊     | 1.20G/2.47G [00:07<00:06, 196Mbyte/s]\n",
      "Writing:  50%|████▉     | 1.23G/2.47G [00:07<00:06, 193Mbyte/s]\n",
      "Writing:  51%|█████     | 1.26G/2.47G [00:07<00:06, 196Mbyte/s]\n",
      "Writing:  52%|█████▏    | 1.29G/2.47G [00:07<00:06, 186Mbyte/s]\n",
      "Writing:  53%|█████▎    | 1.32G/2.47G [00:07<00:06, 189Mbyte/s]\n",
      "Writing:  55%|█████▍    | 1.36G/2.47G [00:08<00:05, 189Mbyte/s]\n",
      "Writing:  56%|█████▌    | 1.38G/2.47G [00:08<00:05, 190Mbyte/s]\n",
      "Writing:  57%|█████▋    | 1.41G/2.47G [00:08<00:05, 192Mbyte/s]\n",
      "Writing:  58%|█████▊    | 1.44G/2.47G [00:08<00:05, 191Mbyte/s]\n",
      "Writing:  60%|█████▉    | 1.48G/2.47G [00:08<00:06, 150Mbyte/s]\n",
      "Writing:  61%|██████    | 1.50G/2.47G [00:08<00:06, 153Mbyte/s]\n",
      "Writing:  62%|██████▏   | 1.53G/2.47G [00:09<00:06, 138Mbyte/s]\n",
      "Writing:  63%|██████▎   | 1.57G/2.47G [00:09<00:07, 128Mbyte/s]\n",
      "Writing:  65%|██████▍   | 1.60G/2.47G [00:09<00:06, 125Mbyte/s]\n",
      "Writing:  65%|██████▌   | 1.62G/2.47G [00:09<00:07, 120Mbyte/s]\n",
      "Writing:  67%|██████▋   | 1.65G/2.47G [00:10<00:06, 118Mbyte/s]\n",
      "Writing:  68%|██████▊   | 1.69G/2.47G [00:10<00:06, 123Mbyte/s]\n",
      "Writing:  70%|██████▉   | 1.72G/2.47G [00:10<00:05, 126Mbyte/s]\n",
      "Writing:  70%|███████   | 1.74G/2.47G [00:10<00:05, 134Mbyte/s]\n",
      "Writing:  72%|███████▏  | 1.78G/2.47G [00:11<00:05, 134Mbyte/s]\n",
      "Writing:  73%|███████▎  | 1.81G/2.47G [00:11<00:05, 132Mbyte/s]\n",
      "Writing:  75%|███████▍  | 1.84G/2.47G [00:11<00:04, 128Mbyte/s]\n",
      "Writing:  75%|███████▌  | 1.86G/2.47G [00:11<00:04, 134Mbyte/s]\n",
      "Writing:  77%|███████▋  | 1.90G/2.47G [00:12<00:04, 130Mbyte/s]\n",
      "Writing:  78%|███████▊  | 1.93G/2.47G [00:12<00:04, 132Mbyte/s]\n",
      "Writing:  79%|███████▉  | 1.96G/2.47G [00:12<00:03, 139Mbyte/s]\n",
      "Writing:  80%|████████  | 1.99G/2.47G [00:12<00:03, 146Mbyte/s]\n",
      "Writing:  82%|████████▏ | 2.02G/2.47G [00:12<00:02, 156Mbyte/s]\n",
      "Writing:  83%|████████▎ | 2.05G/2.47G [00:13<00:02, 145Mbyte/s]\n",
      "Writing:  84%|████████▍ | 2.09G/2.47G [00:13<00:02, 152Mbyte/s]\n",
      "Writing:  85%|████████▌ | 2.11G/2.47G [00:13<00:02, 159Mbyte/s]\n",
      "Writing:  87%|████████▋ | 2.14G/2.47G [00:13<00:01, 170Mbyte/s]\n",
      "Writing:  88%|████████▊ | 2.17G/2.47G [00:13<00:01, 173Mbyte/s]\n",
      "Writing:  89%|████████▉ | 2.21G/2.47G [00:13<00:01, 178Mbyte/s]\n",
      "Writing:  90%|█████████ | 2.23G/2.47G [00:14<00:01, 180Mbyte/s]\n",
      "Writing:  92%|█████████▏| 2.26G/2.47G [00:14<00:01, 175Mbyte/s]\n",
      "Writing:  93%|█████████▎| 2.30G/2.47G [00:14<00:01, 160Mbyte/s]\n",
      "Writing:  94%|█████████▍| 2.33G/2.47G [00:14<00:00, 163Mbyte/s]\n",
      "Writing:  96%|█████████▋| 2.38G/2.47G [00:14<00:00, 183Mbyte/s]\n",
      "Writing:  98%|█████████▊| 2.42G/2.47G [00:15<00:00, 190Mbyte/s]\n",
      "Writing:  99%|█████████▉| 2.45G/2.47G [00:15<00:00, 182Mbyte/s]\n",
      "Writing: 100%|██████████| 2.47G/2.47G [00:15<00:00, 155Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to llama3-finetuned-merged\\Llama-3.2-1B-Instruct-F16.gguf\n"
     ]
    }
   ],
   "source": [
    "# Conversion en gguf\n",
    "!python ./llama.cpp/convert_hf_to_gguf.py ./llama3-finetuned-merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Conversion du Modèle vers Ollama**\n",
    "\n",
    "Après avoir fine-tuné et fusionné notre modèle **LLAMA 3**, nous pouvons maintenant le convertir au format **Ollama** pour une exécution optimisée sur **CPU** et **GPU**.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 **Pourquoi utiliser Ollama ?**\n",
    "**Ollama** est une plateforme permettant d’exécuter des modèles **LLM optimisés** sur des **machines locales**, avec une gestion efficace de la mémoire et du temps d’inférence.\n",
    "\n",
    "Avantages :\n",
    "- Exécution **rapide et optimisée** sur CPU/GPU  \n",
    "- Compatible avec les fichiers **GGUF** (format performant pour LLMs)  \n",
    "- Facilité d'utilisation avec des **commandes simples**  \n",
    "\n",
    "---\n",
    "\n",
    "## **1. Création du Modèle avec Ollama**\n",
    "Une fois que notre modèle fusionné (`llama3-finetuned-merged`) est prêt, nous devons **créer un modèle Ollama** en utilisant un fichier **Modelfile**.\n",
    "\n",
    "### 🔹 **Commande pour créer le modèle :**\n",
    "```bash\n",
    "ollama create llama-test -f Modelfile\n",
    "```\n",
    "\n",
    "📍 **Explication :**\n",
    "- `ollama create` → Commande pour créer un modèle Ollama  \n",
    "- `llama-test` → Nom du modèle personnalisé  \n",
    "- `-f Modelfile` → Spécifie le fichier `Modelfile` qui contient la configuration du modèle  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. Structure du Fichier `Modelfile`**\n",
    "Le fichier `Modelfile` contient les **instructions** nécessaires pour charger et configurer le modèle dans **Ollama**.\n",
    "\n",
    "```bash\n",
    "FROM ./llama3-finetuned-merged/Llama-3.2-1B-Instruct-F16.gguf\n",
    "\n",
    "PARAMETER stop \"<|endoftext|>\"\n",
    "```\n",
    "\n",
    "📍 **Explication des lignes :**\n",
    "- **`FROM ./llama3-finetuned-merged/Llama-3.2-1B-Instruct-F16.gguf`**  \n",
    "  - Indique que nous utilisons le modèle fine-tuné, converti au format **GGUF**  \n",
    "  - Le format **GGUF** est optimisé pour l'exécution rapide sur CPU/GPU  \n",
    "- **`PARAMETER stop \"<|endoftext|>\"`**  \n",
    "  - Définit le **token de fin de texte** `<|endoftext|>`  \n",
    "  - Permet d'arrêter la génération lorsqu'on atteint ce token  \n",
    "\n",
    "📌 **Pourquoi GGUF ?**  \n",
    "Le format **GGUF** (GPT-Generated Unified Format) est une version optimisée des modèles **GGML**, offrant :\n",
    "- **Meilleure gestion mémoire**\n",
    "- **Performances accrues sur CPU/GPU**\n",
    "- **Compatibilité avec Ollama et llama.cpp**\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Documentation de `Modelfile`**\n",
    "Pour plus de détails sur la configuration de `Modelfile`, consultez la documentation officielle d’Ollama :  \n",
    "🔗 **[Ollama Modelfile Docs](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25lgathering model components ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 0% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 0% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 1% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 2% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 3% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 3% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 4% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 4% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 5% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 6% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 7% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 7% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 8% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 9% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 10% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 11% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 11% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 12% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 13% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 14% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 15% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 16% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 16% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 17% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 18% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 19% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 20% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 21% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 21% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 22% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 23% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 24% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 25% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 26% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 27% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 28% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 29% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 29% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 30% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 31% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 32% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 32% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 32% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 32% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 34% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 35% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 35% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 36% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 37% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 38% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 38% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 38% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 39% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 40% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 41% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 41% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 42% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 43% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 43% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 44% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 44% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 44% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 45% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 46% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 46% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 47% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 48% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 48% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 48% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 48% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 48% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 49% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 50% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 51% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 52% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 53% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 53% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 54% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 54% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 54% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 55% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 56% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 57% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 58% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 59% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 59% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 61% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 62% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 62% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 63% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 65% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 65% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 66% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 67% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 67% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 68% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 69% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 70% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 70% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 71% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 72% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 73% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 74% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 74% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 75% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 76% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 77% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 78% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 79% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 79% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 81% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 82% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 83% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 84% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 85% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 86% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 87% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 88% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 89% ⠇ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 90% ⠏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 91% ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 92% ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 93% ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 94% ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 95% ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 96% ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 97% ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 99% ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 100% \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 100% \n",
      "parsing GGUF ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 100% \n",
      "parsing GGUF ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 100% \n",
      "parsing GGUF ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 100% \n",
      "parsing GGUF ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 100% \n",
      "parsing GGUF \n",
      "using existing layer sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 \n",
      "using existing layer sha256:8c205fccfc04715d8ba26dd04c0e36c8a2d6be37942ff45b09f98f2d7ffc7c6b \n",
      "writing manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 100% \n",
      "parsing GGUF \n",
      "using existing layer sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 \n",
      "using existing layer sha256:8c205fccfc04715d8ba26dd04c0e36c8a2d6be37942ff45b09f98f2d7ffc7c6b \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "# Création du modèle Ollama\n",
    "!ollama create llama-test -f Modelfile"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

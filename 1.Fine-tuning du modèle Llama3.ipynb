{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning du Mod√®le LLAMA 3\n",
    "\n",
    "Ce notebook a pour objectif d'effectuer un fine-tuning du mod√®le **LLAMA 3** en utilisant un dataset personnalis√©.  \n",
    "Nous allons suivre les √©tapes suivantes :\n",
    "\n",
    "1. **Installation des D√©pendances** : Installation des biblioth√®ques n√©cessaires, notamment `transformers`, `bitsandbytes`, `datasets` et `peft`.  \n",
    "2. **Connexion √† Hugging Face et T√©l√©chargement du Mod√®le** : Authentification √† Hugging Face et chargement du mod√®le [`Llama-3.2-1B-Instruct`](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct).  \n",
    "3. **Traitement du Jeu de Donn√©es** : Pr√©traitement des donn√©es, tokenization et mise en forme pour le fine-tuning.  \n",
    "4. **Entra√Ænement du Mod√®le** : Fine-tuning en utilisant **LoRA** et **QLoRA** pour optimiser la consommation m√©moire.  \n",
    "5. **Sauvegarde du Mod√®le et Conversion vers Ollama** : Exportation du mod√®le entra√Æn√© au format compatible **Ollama** pour une utilisation simplifi√©e en local.  \n",
    "\n",
    "Nous utiliserons **`transformers`** de Hugging Face pour le mod√®le, **`datasets`** pour la gestion des donn√©es, et **`peft`** pour des optimisations d'entra√Ænement sur GPU.\n",
    "\n",
    "üí° **Objectif** : Adapter LLAMA 3 √† une t√¢che sp√©cifique tout en optimisant les performances et la consommation m√©moire.\n",
    "\n",
    "üöÄ **Commen√ßons !**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Installation des D√©pendances\n",
    "\n",
    "Avant de commencer le fine-tuning de LLAMA 3, assurez-vous d‚Äôinstaller les logiciels recommand√©s :\n",
    "\n",
    "‚úÖ **Git Bash** ‚Üí [T√©l√©charger ici](https://git-scm.com/downloads)  \n",
    "‚úÖ **Python 3.12** ‚Üí [T√©l√©charger ici](https://www.python.org/downloads/release/python-3128/)  \n",
    "üìå *Pensez √† cocher l‚Äôoption \"Ajouter Python aux variables d‚Äôenvironnement\" lors de l‚Äôinstallation*  \n",
    "‚úÖ **VS Code** ‚Üí [T√©l√©charger ici](https://code.visualstudio.com/Download)  \n",
    "‚úÖ **Ollama** ‚Üí [T√©l√©charger ici](https://ollama.com/download)  \n",
    "\n",
    "---\n",
    "\n",
    "### üìå Ex√©cution du script d‚Äôinstallation\n",
    "\n",
    "Une fois les logiciels install√©s, ouvrez un terminal **Git Bash** et ex√©cutez la commande suivante pour lancer l‚Äôinstallation des d√©pendances et du projet :\n",
    "\n",
    "   ```bash\n",
    "   bash installation.sh\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Connexion √† Hugging Face\n",
    "\n",
    "Pour t√©l√©charger et utiliser le mod√®le **Llama-3.2-1B-Instruct**, vous devez d‚Äôabord configurer votre acc√®s √† Hugging Face.\n",
    "\n",
    "### üîπ √âtape 1 : Cr√©er un compte Hugging Face\n",
    "Si vous n‚Äôavez pas encore de compte, cr√©ez-en un ici :  \n",
    "üëâ [Cr√©er un compte Hugging Face](https://huggingface.co/join)\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ √âtape 2 : G√©n√©rer un Token d‚Äôauthentification\n",
    "1. Rendez-vous dans les param√®tres des tokens :  \n",
    "   üëâ [Cr√©er un token ici](https://huggingface.co/settings/tokens)\n",
    "2. Cliquez sur **New token**.\n",
    "3. Donnez-lui un nom (ex. : `llama3-token`) et attribuez-lui les droits **read**.\n",
    "4. Copiez le token g√©n√©r√©.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ √âtape 3 : Demander l'acc√®s au mod√®le LLAMA 3  \n",
    "Le mod√®le **Llama-3.2-1B-Instruct** n√©cessite une demande d'acc√®s.  \n",
    "Rendez-vous sur cette page et cliquez sur **Request access** :  \n",
    "üëâ [Demander l‚Äôacc√®s au mod√®le](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct)  \n",
    "\n",
    "---\n",
    "\n",
    "### üîπ √âtape 4 : Connexion √† Hugging Face dans le Notebook  \n",
    "\n",
    "Ex√©cutez le code suivant pour vous connecter √† Hugging Face avec votre token dans le terminal :\n",
    "   ```\n",
    "   huggingface-cli login\n",
    "   ```\n",
    "---\n",
    "\n",
    "### üîπ √âtape 5 : T√©l√©chargement et Chargement du Mod√®le LLAMA 3  \n",
    "\n",
    "Une fois connect√©, on peut t√©l√©charger et charger le mod√®le :\n",
    "      ```\n",
    "\n",
    "      from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "      MODEL_NAME = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "\n",
    "      ### Chargement du tokenizer et du mod√®le\n",
    "      tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "      model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\")\n",
    "\n",
    "      print(\"‚úÖ Mod√®le LLAMA 3 charg√© avec succ√®s !\")\n",
    "\n",
    "      ```\n",
    "üöÄ **Votre mod√®le LLAMA 3 est maintenant pr√™t √† √™tre utilis√© !**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Quera\\Desktop\\INRAE-Llama3\\.llama_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Qui-es-tu ?\"\n",
    "\n",
    "# Tokeniser l'entr√©e\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Visualisation des tokens d'entr√©e\n",
    "outputs =model.generate(**inputs, max_length=40)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Visualisation des tokens de sorties\n",
    "outputs =model.generate(**inputs, max_length=40)\n",
    "\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Qui-es-tu? (I am you)\\nI am you\\nI am you\\n\\nUn peu de philosophie\\n\\nUn √™tre humain est une forme de vie complexe, avec des bes'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# D√©codage des tokens de sortie\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('llama3-1B\\\\tokenizer_config.json',\n",
       " 'llama3-1B\\\\special_tokens_map.json',\n",
       " 'llama3-1B\\\\tokenizer.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sauvegarde du mod√®le \n",
    "model.save_pretrained(\"llama3-1B\")\n",
    "tokenizer.save_pretrained(\"llama3-1B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: llama3-1B\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {32}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.float32 --> F16, shape = {2048, 128256}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 131072\n",
      "INFO:hf-to-gguf:gguf: embedding length = 2048\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
      "INFO:hf-to-gguf:gguf: head count = 32\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 1\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 280147 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 128000\n",
      "INFO:gguf.vocab:Setting special token type eos to 128009\n",
      "INFO:gguf.vocab:Setting chat_template to {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- if strftime_now is defined %}\n",
      "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
      "    {%- else %}\n",
      "        {%- set date_string = \"26 Jul 2024\" %}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "        {{- '\"parameters\": ' }}\n",
      "        {{- tool_call.arguments | tojson }}\n",
      "        {{- \"}\" }}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:llama3-1B\\Llama-3.2-1B-Instruct-F16.gguf: n_tensors = 147, total_size = 2.5G\n",
      "\n",
      "Writing:   0%|          | 0.00/2.47G [00:00<?, ?byte/s]\n",
      "Writing:  21%|‚ñà‚ñà‚ñè       | 525M/2.47G [00:02<00:07, 261Mbyte/s]\n",
      "Writing:  23%|‚ñà‚ñà‚ñé       | 559M/2.47G [00:02<00:07, 252Mbyte/s]\n",
      "Writing:  24%|‚ñà‚ñà‚ñç       | 592M/2.47G [00:02<00:07, 248Mbyte/s]\n",
      "Writing:  25%|‚ñà‚ñà‚ñå       | 626M/2.47G [00:02<00:07, 242Mbyte/s]\n",
      "Writing:  28%|‚ñà‚ñà‚ñä       | 681M/2.47G [00:02<00:07, 239Mbyte/s]\n",
      "Writing:  29%|‚ñà‚ñà‚ñâ       | 714M/2.47G [00:02<00:07, 235Mbyte/s]\n",
      "Writing:  30%|‚ñà‚ñà‚ñà       | 748M/2.47G [00:03<00:07, 234Mbyte/s]\n",
      "Writing:  32%|‚ñà‚ñà‚ñà‚ñè      | 802M/2.47G [00:03<00:06, 239Mbyte/s]\n",
      "Writing:  34%|‚ñà‚ñà‚ñà‚ñç      | 836M/2.47G [00:03<00:06, 238Mbyte/s]\n",
      "Writing:  35%|‚ñà‚ñà‚ñà‚ñå      | 869M/2.47G [00:03<00:06, 243Mbyte/s]\n",
      "Writing:  37%|‚ñà‚ñà‚ñà‚ñã      | 924M/2.47G [00:03<00:06, 257Mbyte/s]\n",
      "Writing:  39%|‚ñà‚ñà‚ñà‚ñä      | 957M/2.47G [00:03<00:05, 264Mbyte/s]\n",
      "Writing:  40%|‚ñà‚ñà‚ñà‚ñà      | 991M/2.47G [00:03<00:05, 264Mbyte/s]\n",
      "Writing:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1.05G/2.47G [00:04<00:05, 271Mbyte/s]\n",
      "Writing:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1.08G/2.47G [00:04<00:05, 264Mbyte/s]\n",
      "Writing:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1.11G/2.47G [00:04<00:05, 254Mbyte/s]\n",
      "Writing:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1.17G/2.47G [00:04<00:05, 247Mbyte/s]\n",
      "Writing:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1.20G/2.47G [00:04<00:05, 245Mbyte/s]\n",
      "Writing:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1.23G/2.47G [00:04<00:05, 245Mbyte/s]\n",
      "Writing:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1.29G/2.47G [00:05<00:04, 251Mbyte/s]\n",
      "Writing:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1.32G/2.47G [00:05<00:04, 233Mbyte/s]\n",
      "Writing:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1.36G/2.47G [00:05<00:04, 236Mbyte/s]\n",
      "Writing:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1.41G/2.47G [00:05<00:04, 234Mbyte/s]\n",
      "Writing:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1.44G/2.47G [00:05<00:04, 232Mbyte/s]\n",
      "Writing:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1.48G/2.47G [00:05<00:04, 237Mbyte/s]\n",
      "Writing:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1.53G/2.47G [00:06<00:03, 237Mbyte/s]\n",
      "Writing:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1.57G/2.47G [00:06<00:03, 240Mbyte/s]\n",
      "Writing:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 1.60G/2.47G [00:06<00:03, 240Mbyte/s]\n",
      "Writing:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 1.65G/2.47G [00:06<00:03, 219Mbyte/s]\n",
      "Writing:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 1.69G/2.47G [00:06<00:03, 218Mbyte/s]\n",
      "Writing:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1.72G/2.47G [00:07<00:03, 215Mbyte/s]\n",
      "Writing:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1.78G/2.47G [00:07<00:03, 213Mbyte/s]\n",
      "Writing:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 1.81G/2.47G [00:07<00:03, 210Mbyte/s]\n",
      "Writing:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1.84G/2.47G [00:07<00:03, 194Mbyte/s]\n",
      "Writing:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1.86G/2.47G [00:07<00:03, 176Mbyte/s]\n",
      "Writing:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1.90G/2.47G [00:08<00:03, 171Mbyte/s]\n",
      "Writing:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1.93G/2.47G [00:08<00:03, 173Mbyte/s]\n",
      "Writing:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1.96G/2.47G [00:08<00:03, 165Mbyte/s]\n",
      "Writing:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1.98G/2.47G [00:08<00:02, 166Mbyte/s]\n",
      "Writing:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2.02G/2.47G [00:08<00:02, 165Mbyte/s]\n",
      "Writing:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2.05G/2.47G [00:09<00:02, 169Mbyte/s]\n",
      "Writing:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2.09G/2.47G [00:09<00:02, 180Mbyte/s]\n",
      "Writing:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2.11G/2.47G [00:09<00:02, 180Mbyte/s]\n",
      "Writing:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2.14G/2.47G [00:09<00:01, 195Mbyte/s]\n",
      "Writing:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2.17G/2.47G [00:09<00:01, 205Mbyte/s]\n",
      "Writing:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2.21G/2.47G [00:09<00:01, 205Mbyte/s]\n",
      "Writing:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2.26G/2.47G [00:09<00:00, 223Mbyte/s]\n",
      "Writing:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 2.30G/2.47G [00:10<00:00, 227Mbyte/s]\n",
      "Writing:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 2.33G/2.47G [00:10<00:00, 222Mbyte/s]\n",
      "Writing:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 2.38G/2.47G [00:10<00:00, 213Mbyte/s]\n",
      "Writing:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 2.42G/2.47G [00:10<00:00, 215Mbyte/s]\n",
      "Writing:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 2.45G/2.47G [00:10<00:00, 208Mbyte/s]\n",
      "Writing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.47G/2.47G [00:11<00:00, 209Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to llama3-1B\\Llama-3.2-1B-Instruct-F16.gguf\n"
     ]
    }
   ],
   "source": [
    "# Conversion en gguf pour utilisation dans Ollama\n",
    "\n",
    "!python ./llama.cpp/convert_hf_to_gguf.py ./llama3-1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Traitement du Jeu de Donn√©es\n",
    "\n",
    "## üìå Format des donn√©es  \n",
    "\n",
    "Le mod√®le **LLAMA 3** attend des donn√©es sous un format sp√©cifique.  \n",
    "Nous devons structurer les conversations en **JSON Lines** (`.jsonl`), o√π chaque ligne repr√©sente un √©change entre l‚Äôutilisateur et l‚Äôassistant.\n",
    "\n",
    "Exemple de fichier `data.jsonl` :\n",
    "\n",
    "```json\n",
    "{\"messages\": [{\"role\": \"user\", \"content\": \"Qui es-tu ?\"}, {\"role\": \"assistant\", \"content\": \"Je suis l'assistant de l'INRAE<|endoftext|>\"}]}\n",
    "```\n",
    "\n",
    "### üîπ Explication :  \n",
    "- **`role: user`** ‚Üí Contenu du message de l‚Äôutilisateur  \n",
    "- **`role: assistant`** ‚Üí R√©ponse g√©n√©r√©e par le mod√®le  \n",
    "- **`<|endoftext|>`** ‚Üí **Token de fin de texte** qui marque la fin d‚Äôune conversation  \n",
    "\n",
    "Ce format est essentiel pour que le mod√®le apprenne √† r√©pondre correctement en conservant le contexte.\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ Transformations appliqu√©es  \n",
    "\n",
    "Le mod√®le **LLAMA 3** ne peut pas directement utiliser du texte brut. Nous devons **convertir** les messages en une forme que le mod√®le peut traiter :  \n",
    "- **`input_ids`** : Repr√©sentation des tokens du texte  \n",
    "- **`attention_mask`** : Masque indiquant quelles parties du texte doivent √™tre prises en compte  \n",
    "- **`labels`** : Donn√©es de sortie attendues pour l‚Äôapprentissage  \n",
    "\n",
    "### üìç **Avant transformation :**  \n",
    "Format brut des donn√©es :\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Qui es-tu ?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Je suis l'assistant de l'INRAE<|endoftext|>\"}\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üìç **Apr√®s transformation :**  \n",
    "Format pr√™t pour le fine-tuning du mod√®le :\n",
    "\n",
    "```python\n",
    "{\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Qui es-tu ?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Je suis l'assistant de l'INRAE<|endoftext|>\"}\n",
    "    ],\n",
    "    \"input_ids\": [128000, 62545, 1560, 2442, 84, 949, 30854, 36731, 326, 6, 78191, 409, 326, 6, 691, 5726, 36, 27, 91, 8862, 728, 428, 91, 29, 128009],\n",
    "    \"attention_mask\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
    "    \"labels\": [128000, 62545, 1560, 2442, 84, 949, 30854, 36731, 326, 6, 78191, 409, 326, 6, 691, 5726, 36, 27, 91, 8862, 728, 428, 91, 29, -100]\n",
    "}\n",
    "```\n",
    "\n",
    "### üîπ **Explication des transformations :**  \n",
    "1. **Tokenization** :  \n",
    "   - Le texte est converti en **`input_ids`**, une s√©quence de nombres repr√©sentant chaque mot ou caract√®re.  \n",
    "   - Exemple : `\"Qui es-tu ?\"` ‚Üí `[128000, 62545, 1560, 2442, 84]`  \n",
    "\n",
    "2. **Masking** (**`attention_mask`**) :  \n",
    "   - Indique quelles parties du texte doivent √™tre prises en compte.  \n",
    "   - `1` = Token important, `0` = Token ignor√© (padding).  \n",
    "\n",
    "3. **Labels pour l‚Äôentra√Ænement** (**`labels`**) :  \n",
    "   - M√™me s√©quence que **`input_ids`**, sauf que les tokens d‚Äôentr√©e utilisateur sont remplac√©s par `-100` (pour ignorer leur contribution √† la pr√©diction).  \n",
    "   - Cela permet au mod√®le de **pr√©dire uniquement la r√©ponse** et d‚Äôignorer l‚Äôentr√©e utilisateur.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 4 examples [00:00, 137.62 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 4\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# ‚úÖ Charger le dataset JSONL\n",
    "dataset = load_dataset('json', data_files='./data.jsonl')\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple de donn√©es : {'messages': [{'role': 'user', 'content': 'Qui es-tu ?'}, {'role': 'assistant', 'content': \"Je suis l'assistant de l'INRAE<|endoftext|>\"}]}\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ V√©rifier les cl√©s du dataset\n",
    "print(\"Exemple de donn√©es :\", dataset['train'][0])  # Affiche un exemple pour v√©rifier sa structure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation du token de padding pour Lama3\n",
    "tokenizer.pad_token = tokenizer.eos_token  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 172.96 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple de donn√©es : {'messages': [{'role': 'user', 'content': 'Qui es-tu ?'}, {'role': 'assistant', 'content': \"Je suis l'assistant de l'INRAE<|endoftext|>\"}], 'input_ids': [128000, 62545, 1560, 2442, 84, 949, 30854, 36731, 326, 6, 78191, 409, 326, 6, 691, 5726, 36, 27, 91, 8862, 728, 428, 91, 29, 128009], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], 'labels': [128000, 62545, 1560, 2442, 84, 949, 30854, 36731, 326, 6, 78191, 409, 326, 6, 691, 5726, 36, 27, 91, 8862, 728, 428, 91, 29, -100]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Pr√©traitement des donn√©es d'entrainement\n",
    "def preprocess_function(examples):\n",
    "    # üîπ Construire le texte d'entr√©e √† partir des messages\n",
    "    text_inputs = []\n",
    "    for message_set in examples[\"messages\"]:\n",
    "        text = \"\"\n",
    "        for message in message_set:\n",
    "            text += f\"{message['content']}\"\n",
    "        text_inputs.append(text.strip())\n",
    "\n",
    "    # üîπ Tokenisation\n",
    "    inputs = tokenizer(text_inputs, truncation=True, padding=\"max_length\", max_length=25)\n",
    "\n",
    "    # üîπ Copier input_ids pour labels\n",
    "    inputs[\"labels\"] = inputs[\"input_ids\"].copy()\n",
    "\n",
    "    # üîπ Remplacer les tokens de padding par -100 pour ignorer la perte\n",
    "    padding_token_id = tokenizer.pad_token_id\n",
    "    inputs[\"labels\"] = [\n",
    "        [(label if label != padding_token_id else -100) for label in labels] for labels in inputs[\"labels\"]\n",
    "    ]\n",
    "\n",
    "    return inputs\n",
    "\n",
    "# ‚úÖ Appliquer la transformation\n",
    "dataset = dataset.map(preprocess_function, batched=True)\n",
    "print(\"Exemple de donn√©es :\", dataset['train'][0])  # Affiche un exemple pour v√©rifier sa structure\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Entra√Ænement du Mod√®le\n",
    "\n",
    "Apr√®s avoir pr√©par√© nos donn√©es, nous pouvons maintenant entra√Æner **LLAMA 3**.  \n",
    "Nous utiliserons **LoRA (Low-Rank Adaptation)** pour optimiser l'entra√Ænement tout en **r√©duisant la charge m√©moire**.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå 4.1 Configuration du Fine-Tuning avec LoRA\n",
    "\n",
    "### ‚úÖ **Pourquoi LoRA ?**\n",
    "Le fine-tuning complet d'un mod√®le **LLAMA 3** peut √™tre tr√®s gourmand en ressources GPU.  \n",
    "LoRA permet de **r√©duire la quantit√© de poids entra√Ænables** en n'entra√Ænant que certaines couches du mod√®le.\n",
    "\n",
    "Nous d√©finissons les param√®tres de LoRA avec :\n",
    "\n",
    "```python\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# ‚úÖ Config LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=32, lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "```\n",
    "\n",
    "### üìç **Explication des param√®tres :**\n",
    "- **`r=32`** ‚Üí Taille du rang pour la factorisation des matrices. Plus il est √©lev√©, plus l'entra√Ænement est pr√©cis, mais co√ªteux.  \n",
    "- **`lora_alpha=32`** ‚Üí Facteur de mise √† l'√©chelle des poids LoRA.  \n",
    "- **`target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]`** ‚Üí On applique LoRA uniquement sur certaines couches du mod√®le.  \n",
    "- **`lora_dropout=0.05`** ‚Üí Ajout d'un **dropout** pour √©viter l'overfitting.  \n",
    "- **`task_type=\"CAUSAL_LM\"`** ‚Üí On entra√Æne un mod√®le **causal** (g√©n√©ration de texte).  \n",
    "\n",
    "Nous appliquons ensuite cette configuration au mod√®le :\n",
    "\n",
    "```python\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üìå 4.2 D√©finition des Param√®tres d'Entra√Ænement\n",
    "\n",
    "Nous utilisons **`TrainingArguments`** pour d√©finir les options d'entra√Ænement :\n",
    "\n",
    "```python\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3-finetuned\",\n",
    "    per_device_train_batch_size=5,  # ‚ö†Ô∏è R√©duction de la batch size car CPU limit√©\n",
    "    num_train_epochs=10,  # üîπ 10 √©poques pour un bon fine-tuning\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    learning_rate=2e-3,\n",
    "    gradient_accumulation_steps=90,  # üîπ Augmenter pour r√©duire la charge m√©moire\n",
    "    fp16=False,  # üö´ D√©sactiv√© car inutile sur CPU\n",
    "    bf16=False,\n",
    "    gradient_checkpointing=False,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=100,\n",
    "    save_total_limit=2,  # üîπ Garde seulement les 2 derniers checkpoints\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"tensorboard\",  # üîπ Enregistre les logs pour TensorBoard\n",
    "    torch_compile=False,  # ‚úÖ Optimisation CPU\n",
    "    no_cuda=True  # üö´ D√©sactive l'utilisation du GPU\n",
    ")\n",
    "```\n",
    "\n",
    "### üìç **Explication des param√®tres :**\n",
    "- **`output_dir=\"./llama3-finetuned\"`** ‚Üí Sauvegarde du mod√®le entra√Æn√© ici.  \n",
    "- **`per_device_train_batch_size=5`** ‚Üí On utilise une petite batch size pour √©viter d‚Äô√©puiser la RAM.  \n",
    "- **`num_train_epochs=10`** ‚Üí Nombre total de passes sur les donn√©es.  \n",
    "- **`gradient_accumulation_steps=90`** ‚Üí Permet d‚Äôaccumuler les gradients sur plusieurs batchs avant de mettre √† jour les poids (r√©duit la charge m√©moire).  \n",
    "- **`learning_rate=2e-3`** ‚Üí Taux d‚Äôapprentissage initial.  \n",
    "- **`lr_scheduler_type=\"cosine\"`** ‚Üí Programmation du taux d‚Äôapprentissage en fonction d‚Äôune courbe **cosinus**.  \n",
    "- **`save_total_limit=2`** ‚Üí √âvite d'enregistrer trop de checkpoints pour √©conomiser de l'espace disque.  \n",
    "- **`torch_compile=False`** ‚Üí Optimisation de l‚Äôentra√Ænement pour CPU uniquement.  \n",
    "- **`no_cuda=True`** ‚Üí **For√ßage du CPU** (utile si aucun GPU disponible).  \n",
    "\n",
    "---\n",
    "\n",
    "### üìå 4.3 Pr√©paration des Donn√©es pour l'Entra√Ænement\n",
    "\n",
    "Nous utilisons **`DataCollatorForSeq2Seq`** pour normaliser les donn√©es avant l'entra√Ænement.\n",
    "\n",
    "```python\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# ‚úÖ Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\")\n",
    "```\n",
    "\n",
    "üìç **Pourquoi ?**  \n",
    "- Permet d'ajouter du **padding** pour que toutes les s√©quences aient la m√™me longueur.  \n",
    "- Convertit les donn√©es en **tensors PyTorch** (`return_tensors=\"pt\"`).  \n",
    "\n",
    "---\n",
    "\n",
    "### üìå 4.4 Lancement de l‚ÄôEntra√Ænement  \n",
    "\n",
    "Enfin, nous utilisons **`Trainer`** pour entra√Æner le mod√®le avec nos donn√©es :\n",
    "\n",
    "```python\n",
    "from transformers import Trainer\n",
    "\n",
    "# ‚úÖ Entra√Ænement\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['train'],  # üîπ On utilise le m√™me dataset pour l‚Äô√©valuation (peut √™tre remplac√©)\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "### üìç **Explication :**\n",
    "- **`model=model`** ‚Üí Mod√®le LoRA pr√™t √† √™tre entra√Æn√©.  \n",
    "- **`args=training_args`** ‚Üí Param√®tres d'entra√Ænement d√©finis plus t√¥t.  \n",
    "- **`train_dataset=dataset['train']`** ‚Üí Donn√©es utilis√©es pour l'entra√Ænement.  \n",
    "- **`eval_dataset=dataset['train']`** ‚Üí Ici, on utilise le m√™me dataset pour l‚Äô√©valuation, mais id√©alement il faut un dataset s√©par√©.  \n",
    "- **`data_collator=data_collator`** ‚Üí Regroupe les donn√©es de mani√®re efficace.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------\n",
      "trainable params: 6,815,744 || all params: 1,242,630,144 || trainable%: 0.5485\n",
      "---------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Quera\\Desktop\\Llama3\\.llama\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "c:\\Users\\Quera\\Desktop\\Llama3\\.llama\\Lib\\site-packages\\transformers\\training_args.py:1609: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ü§ó Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:57, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.148244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.027769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.843416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.643522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.435893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.198648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.951529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.705722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.452125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.106727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=2.3531587600708006, metrics={'train_runtime': 60.1425, 'train_samples_per_second': 0.665, 'train_steps_per_second': 0.166, 'total_flos': 7526107054080.0, 'train_loss': 2.3531587600708006, 'epoch': 10.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "# ‚úÖ Config LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=32, lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "\n",
    "print(\"---------------------------------------------------------------\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "print(\"---------------------------------------------------------------\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3-finetuned\",\n",
    "    per_device_train_batch_size=5,  # ‚ö†Ô∏è R√©duire la batch size car CPU est limit√©\n",
    "    num_train_epochs=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    learning_rate=2e-3,\n",
    "    gradient_accumulation_steps=90,  # üîπ Augmenter pour r√©duire la charge m√©moire\n",
    "    fp16=False,  # üö´ D√©sactiver fp16 (inutile sur CPU)\n",
    "    bf16=False,\n",
    "    gradient_checkpointing=False,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=100,\n",
    "    save_total_limit=2,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"tensorboard\",\n",
    "    torch_compile=False,  # ‚úÖ Optimisation CPU\n",
    "    no_cuda=True\n",
    ")\n",
    "\n",
    "\n",
    "# ‚úÖ Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "# ‚úÖ Entra√Ænement\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['train'],\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:48, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.106727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.055463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.946546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.868594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.784750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.658026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.600254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.588399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.586216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.585886</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=0.7959489822387695, metrics={'train_runtime': 51.1493, 'train_samples_per_second': 0.782, 'train_steps_per_second': 0.196, 'total_flos': 7526107054080.0, 'train_loss': 0.7959489822387695, 'epoch': 10.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deuxi√®me s√©rie d'entrainement\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000,  62545,   1560,   2442,     84,    949]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformation des tokens d'entr√©\n",
    "prompt =\"Qui es-tu ?\"\n",
    "input_exemple = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Pr√©diction du mod√®le apr√®s entrainement\n",
    "output = model.generate(**input_exemple, max_length=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Qui es-tu?Je suis l'assistant de l'INRAE<|endoftext|>Je\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transformation des tokens de sortie\n",
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Sauvegarde du Mod√®le et Conversion vers Ollama\n",
    "\n",
    "Une fois l‚Äôentra√Ænement termin√©, nous devons **enregistrer** notre mod√®le pour pouvoir l‚Äôutiliser ult√©rieurement.  \n",
    "\n",
    "Deux √©tapes sont n√©cessaires :\n",
    "1. **Sauvegarde du mod√®le entra√Æn√© avec LoRA**\n",
    "2. **Fusion des poids LoRA avec le mod√®le de base** pour obtenir un mod√®le autonome.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå 5.1 Sauvegarde du Mod√®le LoRA\n",
    "\n",
    "Le mod√®le fine-tun√© avec **LoRA** contient des poids adaptatifs s√©par√©s.  \n",
    "Nous devons sauvegarder :\n",
    "- **Le mod√®le LoRA**\n",
    "- **Le tokenizer** (indispensable pour la g√©n√©ration de texte)\n",
    "\n",
    "```python\n",
    "# Sauvegarde du mod√®le LoRA et du tokenizer\n",
    "model.save_pretrained(\"llama3-finetuned\")\n",
    "tokenizer.save_pretrained(\"llama3-finetuned\")\n",
    "```\n",
    "\n",
    "üìç **Explication :**  \n",
    "- `model.save_pretrained(\"llama3-finetuned\")` ‚Üí Sauvegarde le mod√®le entra√Æn√© avec les poids LoRA.  \n",
    "- `tokenizer.save_pretrained(\"llama3-finetuned\")` ‚Üí Sauvegarde le **tokenizer**, essentiel pour la conversion texte ‚Üí tokens.  \n",
    "\n",
    "> üìù **√Ä noter** : √Ä ce stade, le mod√®le d√©pend encore des poids **LoRA**.  \n",
    "> Il faut maintenant **fusionner ces poids** avec le mod√®le de base.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå 5.2 Fusion des Poids LoRA avec le Mod√®le de Base\n",
    "\n",
    "LoRA ajoute uniquement des **ajustements** aux poids du mod√®le de base.  \n",
    "Si on veut un **mod√®le autonome**, il faut fusionner ces ajustements avec le mod√®le d‚Äôorigine.\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# Chargement du mod√®le de base\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", device_map=\"cpu\")\n",
    "\n",
    "# Chargement du mod√®le LoRA et fusion avec le mod√®le de base\n",
    "peft_model = PeftModel.from_pretrained(base_model, \"llama3-finetuned\")\n",
    "peft_model = peft_model.merge_and_unload()\n",
    "```\n",
    "\n",
    "üìç **Explication :**  \n",
    "1. **On recharge le mod√®le de base** `meta-llama/Llama-3.2-1B-Instruct`  \n",
    "2. **On applique les poids LoRA** avec `PeftModel.from_pretrained(base_model, \"llama3-finetuned\")`  \n",
    "3. **On fusionne les poids** en appelant `merge_and_unload()`, ce qui int√®gre d√©finitivement les ajustements LoRA dans le mod√®le principal.  \n",
    "\n",
    "Apr√®s cette fusion, le mod√®le devient totalement **ind√©pendant de LoRA** et peut √™tre utilis√© comme un mod√®le standard.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå 3. Sauvegarde du Mod√®le Fusionn√©\n",
    "\n",
    "Une fois la fusion termin√©e, nous enregistrons **le mod√®le final** :\n",
    "\n",
    "```python\n",
    "# Sauvegarde du mod√®le fusionn√© (sans LoRA)\n",
    "peft_model.save_pretrained(\"llama3-finetuned-merged\")\n",
    "tokenizer.save_pretrained(\"llama3-finetuned-merged\")\n",
    "\n",
    "print(\"‚úÖ Fusion et sauvegarde du mod√®le termin√© !\")\n",
    "```\n",
    "\n",
    "üìç **Explication :**  \n",
    "- **`peft_model.save_pretrained(\"llama3-finetuned-merged\")`** ‚Üí Sauvegarde le mod√®le final, pr√™t √† √™tre utilis√© **sans d√©pendance LoRA**.  \n",
    "- **`tokenizer.save_pretrained(\"llama3-finetuned-merged\")`** ‚Üí Sauvegarde le **tokenizer** avec le mod√®le fusionn√©.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fusion et sauvegarde du mod√®le termin√© !\n"
     ]
    }
   ],
   "source": [
    "# Sauvegarde du mod√®le LoRA et fusion des poids du mod√®le de base\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "model.save_pretrained(\"llama3-finetuned\")\n",
    "tokenizer.save_pretrained(\"llama3-finetuned\")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", device_map=\"cpu\")\n",
    "peft_model = PeftModel.from_pretrained(base_model, \"llama3-finetuned\")\n",
    "\n",
    "\n",
    "# Fusionner les poids LoRA avec le mod√®le principal\n",
    "peft_model = peft_model.merge_and_unload()\n",
    "\n",
    "# Sauvegarde du mod√®le fusionn√© (sans LoRA)\n",
    "peft_model.save_pretrained(\"llama3-finetuned-merged\")\n",
    "tokenizer.save_pretrained(\"llama3-finetuned-merged\")\n",
    "\n",
    "print(\"‚úÖ Fusion et sauvegarde du mod√®le termin√© !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:hf-to-gguf:Loading model: llama3-finetuned-merged\n",
      "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
      "INFO:hf-to-gguf:Exporting model...\n",
      "INFO:hf-to-gguf:rope_freqs.weight,           torch.float32 --> F32, shape = {32}\n",
      "INFO:hf-to-gguf:gguf: loading model part 'model.safetensors'\n",
      "INFO:hf-to-gguf:token_embd.weight,           torch.float32 --> F16, shape = {2048, 128256}\n",
      "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float32 --> F16, shape = {8192, 2048}\n",
      "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float32 --> F16, shape = {2048, 8192}\n",
      "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float32 --> F16, shape = {2048, 2048}\n",
      "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float32 --> F16, shape = {2048, 512}\n",
      "INFO:hf-to-gguf:output_norm.weight,          torch.float32 --> F32, shape = {2048}\n",
      "INFO:hf-to-gguf:Set meta model\n",
      "INFO:hf-to-gguf:Set model parameters\n",
      "INFO:hf-to-gguf:gguf: context length = 131072\n",
      "INFO:hf-to-gguf:gguf: embedding length = 2048\n",
      "INFO:hf-to-gguf:gguf: feed forward length = 8192\n",
      "INFO:hf-to-gguf:gguf: head count = 32\n",
      "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
      "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
      "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
      "INFO:hf-to-gguf:gguf: file type = 1\n",
      "INFO:hf-to-gguf:Set model tokenizer\n",
      "INFO:gguf.vocab:Adding 280147 merge(s).\n",
      "INFO:gguf.vocab:Setting special token type bos to 128000\n",
      "INFO:gguf.vocab:Setting special token type eos to 128009\n",
      "INFO:gguf.vocab:Setting special token type pad to 128009\n",
      "INFO:gguf.vocab:Setting chat_template to {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- if strftime_now is defined %}\n",
      "        {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
      "    {%- else %}\n",
      "        {%- set date_string = \"26 Jul 2024\" %}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "        {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "        {{- '\"parameters\": ' }}\n",
      "        {{- tool_call.arguments | tojson }}\n",
      "        {{- \"}\" }}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "INFO:hf-to-gguf:Set model quantization version\n",
      "INFO:gguf.gguf_writer:Writing the following files:\n",
      "INFO:gguf.gguf_writer:llama3-finetuned-merged\\Llama-3.2-1B-Instruct-F16.gguf: n_tensors = 147, total_size = 2.5G\n",
      "\n",
      "Writing:   0%|          | 0.00/2.47G [00:00<?, ?byte/s]\n",
      "Writing:  21%|‚ñà‚ñà‚ñè       | 525M/2.47G [00:03<00:13, 144Mbyte/s]\n",
      "Writing:  23%|‚ñà‚ñà‚ñé       | 559M/2.47G [00:03<00:13, 143Mbyte/s]\n",
      "Writing:  24%|‚ñà‚ñà‚ñç       | 592M/2.47G [00:04<00:12, 148Mbyte/s]\n",
      "Writing:  25%|‚ñà‚ñà‚ñå       | 626M/2.47G [00:04<00:12, 152Mbyte/s]\n",
      "Writing:  28%|‚ñà‚ñà‚ñä       | 681M/2.47G [00:04<00:11, 162Mbyte/s]\n",
      "Writing:  29%|‚ñà‚ñà‚ñâ       | 714M/2.47G [00:04<00:10, 167Mbyte/s]\n",
      "Writing:  30%|‚ñà‚ñà‚ñà       | 748M/2.47G [00:04<00:10, 171Mbyte/s]\n",
      "Writing:  31%|‚ñà‚ñà‚ñà       | 769M/2.47G [00:04<00:09, 174Mbyte/s]\n",
      "Writing:  32%|‚ñà‚ñà‚ñà‚ñè      | 802M/2.47G [00:05<00:09, 182Mbyte/s]\n",
      "Writing:  34%|‚ñà‚ñà‚ñà‚ñç      | 836M/2.47G [00:05<00:08, 186Mbyte/s]\n",
      "Writing:  35%|‚ñà‚ñà‚ñà‚ñå      | 869M/2.47G [00:05<00:08, 190Mbyte/s]\n",
      "Writing:  36%|‚ñà‚ñà‚ñà‚ñå      | 890M/2.47G [00:05<00:08, 193Mbyte/s]\n",
      "Writing:  37%|‚ñà‚ñà‚ñà‚ñã      | 924M/2.47G [00:05<00:08, 191Mbyte/s]\n",
      "Writing:  39%|‚ñà‚ñà‚ñà‚ñä      | 957M/2.47G [00:05<00:07, 195Mbyte/s]\n",
      "Writing:  40%|‚ñà‚ñà‚ñà‚ñà      | 991M/2.47G [00:06<00:07, 194Mbyte/s]\n",
      "Writing:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 1.05G/2.47G [00:06<00:07, 189Mbyte/s]\n",
      "Writing:  44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 1.08G/2.47G [00:06<00:07, 189Mbyte/s]\n",
      "Writing:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 1.11G/2.47G [00:06<00:07, 191Mbyte/s]\n",
      "Writing:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 1.17G/2.47G [00:07<00:06, 195Mbyte/s]\n",
      "Writing:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 1.20G/2.47G [00:07<00:06, 196Mbyte/s]\n",
      "Writing:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 1.23G/2.47G [00:07<00:06, 193Mbyte/s]\n",
      "Writing:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1.26G/2.47G [00:07<00:06, 196Mbyte/s]\n",
      "Writing:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 1.29G/2.47G [00:07<00:06, 186Mbyte/s]\n",
      "Writing:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 1.32G/2.47G [00:07<00:06, 189Mbyte/s]\n",
      "Writing:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 1.36G/2.47G [00:08<00:05, 189Mbyte/s]\n",
      "Writing:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 1.38G/2.47G [00:08<00:05, 190Mbyte/s]\n",
      "Writing:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 1.41G/2.47G [00:08<00:05, 192Mbyte/s]\n",
      "Writing:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 1.44G/2.47G [00:08<00:05, 191Mbyte/s]\n",
      "Writing:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 1.48G/2.47G [00:08<00:06, 150Mbyte/s]\n",
      "Writing:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 1.50G/2.47G [00:08<00:06, 153Mbyte/s]\n",
      "Writing:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 1.53G/2.47G [00:09<00:06, 138Mbyte/s]\n",
      "Writing:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 1.57G/2.47G [00:09<00:07, 128Mbyte/s]\n",
      "Writing:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 1.60G/2.47G [00:09<00:06, 125Mbyte/s]\n",
      "Writing:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 1.62G/2.47G [00:09<00:07, 120Mbyte/s]\n",
      "Writing:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 1.65G/2.47G [00:10<00:06, 118Mbyte/s]\n",
      "Writing:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 1.69G/2.47G [00:10<00:06, 123Mbyte/s]\n",
      "Writing:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1.72G/2.47G [00:10<00:05, 126Mbyte/s]\n",
      "Writing:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1.74G/2.47G [00:10<00:05, 134Mbyte/s]\n",
      "Writing:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 1.78G/2.47G [00:11<00:05, 134Mbyte/s]\n",
      "Writing:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 1.81G/2.47G [00:11<00:05, 132Mbyte/s]\n",
      "Writing:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 1.84G/2.47G [00:11<00:04, 128Mbyte/s]\n",
      "Writing:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 1.86G/2.47G [00:11<00:04, 134Mbyte/s]\n",
      "Writing:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 1.90G/2.47G [00:12<00:04, 130Mbyte/s]\n",
      "Writing:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1.93G/2.47G [00:12<00:04, 132Mbyte/s]\n",
      "Writing:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1.96G/2.47G [00:12<00:03, 139Mbyte/s]\n",
      "Writing:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 1.99G/2.47G [00:12<00:03, 146Mbyte/s]\n",
      "Writing:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 2.02G/2.47G [00:12<00:02, 156Mbyte/s]\n",
      "Writing:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 2.05G/2.47G [00:13<00:02, 145Mbyte/s]\n",
      "Writing:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 2.09G/2.47G [00:13<00:02, 152Mbyte/s]\n",
      "Writing:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 2.11G/2.47G [00:13<00:02, 159Mbyte/s]\n",
      "Writing:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 2.14G/2.47G [00:13<00:01, 170Mbyte/s]\n",
      "Writing:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 2.17G/2.47G [00:13<00:01, 173Mbyte/s]\n",
      "Writing:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 2.21G/2.47G [00:13<00:01, 178Mbyte/s]\n",
      "Writing:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 2.23G/2.47G [00:14<00:01, 180Mbyte/s]\n",
      "Writing:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 2.26G/2.47G [00:14<00:01, 175Mbyte/s]\n",
      "Writing:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 2.30G/2.47G [00:14<00:01, 160Mbyte/s]\n",
      "Writing:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 2.33G/2.47G [00:14<00:00, 163Mbyte/s]\n",
      "Writing:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 2.38G/2.47G [00:14<00:00, 183Mbyte/s]\n",
      "Writing:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 2.42G/2.47G [00:15<00:00, 190Mbyte/s]\n",
      "Writing:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 2.45G/2.47G [00:15<00:00, 182Mbyte/s]\n",
      "Writing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.47G/2.47G [00:15<00:00, 155Mbyte/s]\n",
      "INFO:hf-to-gguf:Model successfully exported to llama3-finetuned-merged\\Llama-3.2-1B-Instruct-F16.gguf\n"
     ]
    }
   ],
   "source": [
    "# Conversion en gguf\n",
    "!python ./llama.cpp/convert_hf_to_gguf.py ./llama3-finetuned-merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Conversion du Mod√®le vers Ollama**\n",
    "\n",
    "Apr√®s avoir fine-tun√© et fusionn√© notre mod√®le **LLAMA 3**, nous pouvons maintenant le convertir au format **Ollama** pour une ex√©cution optimis√©e sur **CPU** et **GPU**.\n",
    "\n",
    "---\n",
    "\n",
    "### üìå **Pourquoi utiliser Ollama ?**\n",
    "**Ollama** est une plateforme permettant d‚Äôex√©cuter des mod√®les **LLM optimis√©s** sur des **machines locales**, avec une gestion efficace de la m√©moire et du temps d‚Äôinf√©rence.\n",
    "\n",
    "Avantages :\n",
    "- Ex√©cution **rapide et optimis√©e** sur CPU/GPU  \n",
    "- Compatible avec les fichiers **GGUF** (format performant pour LLMs)  \n",
    "- Facilit√© d'utilisation avec des **commandes simples**  \n",
    "\n",
    "---\n",
    "\n",
    "## **1. Cr√©ation du Mod√®le avec Ollama**\n",
    "Une fois que notre mod√®le fusionn√© (`llama3-finetuned-merged`) est pr√™t, nous devons **cr√©er un mod√®le Ollama** en utilisant un fichier **Modelfile**.\n",
    "\n",
    "### üîπ **Commande pour cr√©er le mod√®le :**\n",
    "```bash\n",
    "ollama create llama-test -f Modelfile\n",
    "```\n",
    "\n",
    "üìç **Explication :**\n",
    "- `ollama create` ‚Üí Commande pour cr√©er un mod√®le Ollama  \n",
    "- `llama-test` ‚Üí Nom du mod√®le personnalis√©  \n",
    "- `-f Modelfile` ‚Üí Sp√©cifie le fichier `Modelfile` qui contient la configuration du mod√®le  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. Structure du Fichier `Modelfile`**\n",
    "Le fichier `Modelfile` contient les **instructions** n√©cessaires pour charger et configurer le mod√®le dans **Ollama**.\n",
    "\n",
    "```bash\n",
    "FROM ./llama3-finetuned-merged/Llama-3.2-1B-Instruct-F16.gguf\n",
    "\n",
    "PARAMETER stop \"<|endoftext|>\"\n",
    "```\n",
    "\n",
    "üìç **Explication des lignes :**\n",
    "- **`FROM ./llama3-finetuned-merged/Llama-3.2-1B-Instruct-F16.gguf`**  \n",
    "  - Indique que nous utilisons le mod√®le fine-tun√©, converti au format **GGUF**  \n",
    "  - Le format **GGUF** est optimis√© pour l'ex√©cution rapide sur CPU/GPU  \n",
    "- **`PARAMETER stop \"<|endoftext|>\"`**  \n",
    "  - D√©finit le **token de fin de texte** `<|endoftext|>`  \n",
    "  - Permet d'arr√™ter la g√©n√©ration lorsqu'on atteint ce token  \n",
    "\n",
    "üìå **Pourquoi GGUF ?**  \n",
    "Le format **GGUF** (GPT-Generated Unified Format) est une version optimis√©e des mod√®les **GGML**, offrant :\n",
    "- **Meilleure gestion m√©moire**\n",
    "- **Performances accrues sur CPU/GPU**\n",
    "- **Compatibilit√© avec Ollama et llama.cpp**\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Documentation de `Modelfile`**\n",
    "Pour plus de d√©tails sur la configuration de `Modelfile`, consultez la documentation officielle d‚ÄôOllama :  \n",
    "üîó **[Ollama Modelfile Docs](https://github.com/ollama/ollama/blob/main/docs/modelfile.md)**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[?25lgathering model components ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 0% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 0% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 1% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 2% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 3% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 3% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 4% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 4% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 5% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 6% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 7% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 7% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 8% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 9% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 10% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 11% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 11% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 12% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 13% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 14% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 15% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 16% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 16% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 17% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 18% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 19% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 20% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 21% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 21% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 22% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 23% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 24% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 25% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 26% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 27% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 28% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 29% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 29% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 30% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 31% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 32% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 32% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 32% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 32% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 34% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 35% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 35% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 36% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 37% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 38% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 38% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 38% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 39% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 40% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 41% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 41% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 42% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 43% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 43% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 44% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 44% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 44% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 45% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 46% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 46% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 47% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 48% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 48% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 48% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 48% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 48% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 49% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 50% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 51% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 52% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 53% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 53% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 54% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 54% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 54% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 55% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 56% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 57% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 58% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 59% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 59% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 61% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 62% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 62% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 63% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 65% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 65% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 66% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 67% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 67% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 68% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 69% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 70% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 70% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 71% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 72% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 73% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 74% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 74% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 75% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 76% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 77% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 78% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 79% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 79% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 81% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 82% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 83% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 84% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 85% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 86% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 87% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 88% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 89% ‚†á \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 90% ‚†è \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 91% ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 92% ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 93% ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 94% ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 95% ‚†º \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 96% ‚†¥ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 97% ‚†¶ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 99% ‚†ß \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 100% \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 100% \n",
      "parsing GGUF ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 100% \n",
      "parsing GGUF ‚†ô \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 100% \n",
      "parsing GGUF ‚†π \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 100% \n",
      "parsing GGUF ‚†∏ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 100% \n",
      "parsing GGUF \n",
      "using existing layer sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 \n",
      "using existing layer sha256:8c205fccfc04715d8ba26dd04c0e36c8a2d6be37942ff45b09f98f2d7ffc7c6b \n",
      "writing manifest ‚†ã \u001b[?25h\u001b[?25l\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1G\u001b[A\u001b[2K\u001b[1Ggathering model components \n",
      "copying file sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 100% \n",
      "parsing GGUF \n",
      "using existing layer sha256:b8be223ca16dbfa80d145efef5a1b00457438ab00649a23e185d71146f8dd1b0 \n",
      "using existing layer sha256:8c205fccfc04715d8ba26dd04c0e36c8a2d6be37942ff45b09f98f2d7ffc7c6b \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "# Cr√©ation du mod√®le Ollama\n",
    "!ollama create llama-test -f Modelfile"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

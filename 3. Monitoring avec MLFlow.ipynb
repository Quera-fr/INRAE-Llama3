{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **MLOps avec MLFlow**\n",
    "\n",
    "<img src='https://www.databricks.com/sites/default/files/mlflow.png'>\n",
    "\n",
    "Dans ce notebook, nous allons utiliser la bibliothèque Mlflow pour suivre les expériences de machine learning.\n",
    "\n",
    "Nous allons :\n",
    "- créer un projet Mlflow\n",
    "- créer une expérience\n",
    "- créer des runs\n",
    "- suivre les métriques, les paramètres et les artefacts\n",
    "- visualiser les résultats dans l'interface Mlflow\n",
    "- enregistrer un modèle puis le charger dans un autre notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mlflow==2.16.1\n",
      "  Downloading mlflow-2.16.1-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting mlflow-skinny==2.16.1 (from mlflow==2.16.1)\n",
      "  Downloading mlflow_skinny-2.16.1-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: Flask<4 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from mlflow==2.16.1) (3.1.0)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from mlflow==2.16.1) (1.14.0)\n",
      "Requirement already satisfied: docker<8,>=4.0.0 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from mlflow==2.16.1) (7.1.0)\n",
      "Collecting graphene<4 (from mlflow==2.16.1)\n",
      "  Using cached graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from mlflow==2.16.1) (3.7)\n",
      "Collecting matplotlib<4 (from mlflow==2.16.1)\n",
      "  Downloading matplotlib-3.10.1-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<3 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from mlflow==2.16.1) (1.26.4)\n",
      "Requirement already satisfied: pandas<3 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from mlflow==2.16.1) (2.2.3)\n",
      "Collecting pyarrow<18,>=4.0.0 (from mlflow==2.16.1)\n",
      "  Downloading pyarrow-17.0.0-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: scikit-learn<2 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from mlflow==2.16.1) (1.6.1)\n",
      "Requirement already satisfied: scipy<2 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from mlflow==2.16.1) (1.15.2)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from mlflow==2.16.1) (2.0.38)\n",
      "Requirement already satisfied: Jinja2<4,>=3.0 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from mlflow==2.16.1) (3.1.6)\n",
      "Collecting waitress<4 (from mlflow==2.16.1)\n",
      "  Using cached waitress-3.0.2-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from mlflow-skinny==2.16.1->mlflow==2.16.1) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from mlflow-skinny==2.16.1->mlflow==2.16.1) (8.1.8)\n",
      "Collecting cloudpickle<4 (from mlflow-skinny==2.16.1->mlflow==2.16.1)\n",
      "  Using cached cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.16.1->mlflow==2.16.1)\n",
      "  Downloading databricks_sdk-0.45.0-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from mlflow-skinny==2.16.1->mlflow==2.16.1) (3.1.44)\n",
      "Requirement already satisfied: importlib-metadata!=4.7.0,<9,>=3.7.0 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from mlflow-skinny==2.16.1->mlflow==2.16.1) (8.4.0)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from mlflow-skinny==2.16.1->mlflow==2.16.1) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from mlflow-skinny==2.16.1->mlflow==2.16.1) (1.27.0)\n",
      "Requirement already satisfied: packaging<25 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from mlflow-skinny==2.16.1->mlflow==2.16.1) (23.2)\n",
      "Requirement already satisfied: protobuf<6,>=3.12.0 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from mlflow-skinny==2.16.1->mlflow==2.16.1) (4.25.6)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from mlflow-skinny==2.16.1->mlflow==2.16.1) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.17.3 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from mlflow-skinny==2.16.1->mlflow==2.16.1) (2.32.3)\n",
      "Collecting sqlparse<1,>=0.4.0 (from mlflow-skinny==2.16.1->mlflow==2.16.1)\n",
      "  Using cached sqlparse-0.5.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: Mako in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from alembic!=1.10.0,<2->mlflow==2.16.1) (1.3.9)\n",
      "Requirement already satisfied: typing-extensions>=4 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from alembic!=1.10.0,<2->mlflow==2.16.1) (4.12.2)\n",
      "Requirement already satisfied: pywin32>=304 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from docker<8,>=4.0.0->mlflow==2.16.1) (309)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from docker<8,>=4.0.0->mlflow==2.16.1) (2.3.0)\n",
      "Requirement already satisfied: Werkzeug>=3.1 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from Flask<4->mlflow==2.16.1) (3.1.3)\n",
      "Requirement already satisfied: itsdangerous>=2.2 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from Flask<4->mlflow==2.16.1) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.9 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from Flask<4->mlflow==2.16.1) (1.9.0)\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow==2.16.1)\n",
      "  Using cached graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow==2.16.1)\n",
      "  Using cached graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.0 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from graphene<4->mlflow==2.16.1) (2.9.0.post0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from Jinja2<4,>=3.0->mlflow==2.16.1) (3.0.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib<4->mlflow==2.16.1)\n",
      "  Using cached contourpy-1.3.1-cp311-cp311-win_amd64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib<4->mlflow==2.16.1)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from matplotlib<4->mlflow==2.16.1) (4.56.0)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib<4->mlflow==2.16.1)\n",
      "  Using cached kiwisolver-1.4.8-cp311-cp311-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from matplotlib<4->mlflow==2.16.1) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from matplotlib<4->mlflow==2.16.1) (3.2.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from pandas<3->mlflow==2.16.1) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from pandas<3->mlflow==2.16.1) (2025.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from scikit-learn<2->mlflow==2.16.1) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from scikit-learn<2->mlflow==2.16.1) (3.5.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from sqlalchemy<3,>=1.4.0->mlflow==2.16.1) (3.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from click<9,>=7.0->mlflow-skinny==2.16.1->mlflow==2.16.1) (0.4.6)\n",
      "Requirement already satisfied: google-auth~=2.0 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.1->mlflow==2.16.1) (2.38.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from gitpython<4,>=3.1.9->mlflow-skinny==2.16.1->mlflow==2.16.1) (4.0.12)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from importlib-metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.16.1->mlflow==2.16.1) (3.21.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.16.1->mlflow==2.16.1) (1.2.18)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.16.1->mlflow==2.16.1) (0.48b0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow==2.16.1) (1.17.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.16.1->mlflow==2.16.1) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.16.1->mlflow==2.16.1) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from requests<3,>=2.17.3->mlflow-skinny==2.16.1->mlflow==2.16.1) (2025.1.31)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.16.1->mlflow==2.16.1) (1.17.2)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.16.1->mlflow==2.16.1) (5.0.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.1->mlflow==2.16.1) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.1->mlflow==2.16.1) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\quera\\desktop\\inrae-llama3-main\\.llama_env\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.16.1->mlflow==2.16.1) (0.4.8)\n",
      "Downloading mlflow-2.16.1-py3-none-any.whl (26.7 MB)\n",
      "   ---------------------------------------- 0.0/26.7 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 12.6/26.7 MB 65.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 24.1/26.7 MB 61.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.7/26.7 MB 49.6 MB/s eta 0:00:00\n",
      "Downloading mlflow_skinny-2.16.1-py3-none-any.whl (5.6 MB)\n",
      "   ---------------------------------------- 0.0/5.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 5.6/5.6 MB 68.4 MB/s eta 0:00:00\n",
      "Using cached graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
      "Downloading matplotlib-3.10.1-cp311-cp311-win_amd64.whl (8.1 MB)\n",
      "   ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 8.1/8.1 MB 70.9 MB/s eta 0:00:00\n",
      "Downloading pyarrow-17.0.0-cp311-cp311-win_amd64.whl (25.2 MB)\n",
      "   ---------------------------------------- 0.0/25.2 MB ? eta -:--:--\n",
      "   ------------------------------ --------- 19.1/25.2 MB 93.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.2/25.2 MB 69.1 MB/s eta 0:00:00\n",
      "Using cached waitress-3.0.2-py3-none-any.whl (56 kB)\n",
      "Using cached cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Using cached contourpy-1.3.1-cp311-cp311-win_amd64.whl (219 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading databricks_sdk-0.45.0-py3-none-any.whl (672 kB)\n",
      "   ---------------------------------------- 0.0/672.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 672.8/672.8 kB 27.4 MB/s eta 0:00:00\n",
      "Using cached graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
      "Using cached graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Using cached kiwisolver-1.4.8-cp311-cp311-win_amd64.whl (71 kB)\n",
      "Using cached sqlparse-0.5.3-py3-none-any.whl (44 kB)\n",
      "Installing collected packages: waitress, sqlparse, pyarrow, kiwisolver, graphql-core, cycler, contourpy, cloudpickle, matplotlib, graphql-relay, graphene, databricks-sdk, mlflow-skinny, mlflow\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 19.0.1\n",
      "    Uninstalling pyarrow-19.0.1:\n",
      "      Successfully uninstalled pyarrow-19.0.1\n",
      "Successfully installed cloudpickle-3.1.1 contourpy-1.3.1 cycler-0.12.1 databricks-sdk-0.45.0 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 kiwisolver-1.4.8 matplotlib-3.10.1 mlflow-2.16.1 mlflow-skinny-2.16.1 pyarrow-17.0.0 sqlparse-0.5.3 waitress-3.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Quera\\Desktop\\INRAE-Llama3-main\\.llama_env\\Lib\\site-packages\\~yarrow'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "# Installation des dépendances\n",
    "!pip install mlflow==2.16.1 torchvision boto3\n",
    "\n",
    "\n",
    "# Démarrage du server mlflow \n",
    "# mlflow server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from torch import float16\n",
    "model_name = 'gp2'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\",).save_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\").save_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/12 07:48:40 INFO mlflow.tracking.fluent: Experiment with name 'INRAE - Kevin Duranty' does not exist. Creating a new experiment.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import os \n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = '...'\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = '...'\n",
    "\n",
    "mlflow.set_tracking_uri(\"https://inrae-mlflow-server-6c1614130433.herokuapp.com/\")\n",
    "\n",
    "# Configuration d'une expérience (création si elle n'existe pas) : mlflow.set_experiment(\"Crédit Agricol\")\n",
    "experiment = mlflow.set_experiment('INRAE - Kevin Duranty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "2025/03/12 07:49:23 WARNING mlflow.transformers: The model card could not be retrieved from the hub due to 404 Client Error. (Request ID: Root=1-67d12e72-2a96f13b7e919647559e354a;15226e19-ff12-4f25-963e-f49f444f2e2f)\n",
      "\n",
      "Repository Not Found for url: https://huggingface.co/gp2/resolve/main/README.md.\n",
      "Please make sure you specified the correct `repo_id` and `repo_type`.\n",
      "If you are trying to access a private or gated repo, make sure you are authenticated.\n",
      "2025/03/12 07:49:23 WARNING mlflow.transformers: Unable to find license information for this model. Please verify permissible usage for the model you are storing prior to use.\n",
      "2025/03/12 07:49:23 WARNING mlflow.utils.environment: On Windows, timeout is not supported for model requirement inference. Therefore, the operation is not bound by a timeout and may hang indefinitely. If it hangs, please consider specifying the signature manually.\n",
      "Successfully registered model 'GPT2'.\n",
      "2025/03/12 07:51:04 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: GPT2, version 1\n",
      "Created version '1' of model 'GPT2'.\n",
      "2025/03/12 07:51:04 INFO mlflow.tracking._tracking_service.client: 🏃 View run GPT2 at: https://inrae-mlflow-server-6c1614130433.herokuapp.com/#/experiments/2/runs/71489f463ec2425fae384b36e79e9ae3.\n",
      "2025/03/12 07:51:04 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: https://inrae-mlflow-server-6c1614130433.herokuapp.com/#/experiments/2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle sauvegardé et logué avec MLflow !\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "with mlflow.start_run(experiment_id = experiment.experiment_id, run_name=\"GPT2\"):\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # Log des métriques\n",
    "    mlflow.log_metric(\"train_score\", 0.567 )\n",
    "\n",
    "\n",
    "    # Créer une pipeline pour la génération de texte\n",
    "    text_generation_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    # Log du modèle en précisant le type de tâche\n",
    "    mlflow.transformers.log_model(\n",
    "        transformers_model=text_generation_pipeline,\n",
    "        artifact_path=\"model\",\n",
    "        task=\"text-generation\",\n",
    "        registered_model_name='GPT2'\n",
    "    )\n",
    "    print(\"Modèle sauvegardé et logué avec MLflow !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/03/11 20:00:05 INFO mlflow.tracking._tracking_service.client: 🏃 View run llama3-1B at: https://inrae-mlflow-server-6c1614130433.herokuapp.com/#/experiments/1/runs/75e803e735384d49b2feb1252a564d65.\n",
      "2025/03/11 20:00:05 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: https://inrae-mlflow-server-6c1614130433.herokuapp.com/#/experiments/1.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from torch import float16\n",
    "\n",
    "\n",
    "with mlflow.start_run(experiment_id = experiment.experiment_id, run_name=\"GPT2\"):\n",
    "    model_name = \"gpt2\"\n",
    "    mlflow.log_artifact(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Bonjour: As you've seen from the videos, there are some things that will keep people away. For instance, if you've got a guy just hanging out and you don't want him to be able to stop and ask him for directions\"}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generation_pipeline(\"Bonjour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 13/13 [00:04<00:00,  2.71it/s]\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "logged_model = 'runs:/71489f463ec2425fae384b36e79e9ae3/model'\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Bonjour, France. We may be proud of being the youngest and fattest city in Europe; but if we were now grown-up, we should be ashamed to be part of them—but where are we going to go? —']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict('Bonjour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 13/13 [00:04<00:00,  2.71it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Quera\\\\Desktop\\\\INRAE-Llama3-main\\\\downloaded_model\\\\model'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "mlflow.artifacts.download_artifacts(\n",
    "    artifact_uri=\"s3://inrae-mlflow/storage/2/71489f463ec2425fae384b36e79e9ae3/artifacts/model\",\n",
    "    dst_path=\"./downloaded_model\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('downloaded_model/model/model')\n",
    "tokenizer = AutoTokenizer.from_pretrained('downloaded_model/model/components/tokenizer')\n",
    "\n",
    "text_generation_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Bonjour, the name is used because it is very well known for its sour fruit and its flavour. It is commonly mentioned in the UK as having a sour flavour for good health reasons. It often has been regarded as an excellent substitute for plain'}]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_generation_pipeline('Bonjour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

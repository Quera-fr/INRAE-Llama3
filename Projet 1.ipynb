{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Projet 1 : Fine-tuning d‚Äôun Mod√®le LLAMA 3**  \n",
    "\n",
    "### üéØ **Objectif**  \n",
    "L‚Äôobjectif de cet exercice est de **reprendre toutes les √©tapes** du notebook pour fine-tuner un mod√®le **LLAMA 3**, afin qu‚Äôil puisse r√©pondre √† la question :  \n",
    "**\"Quelles sont les champs d'activit√© de l'INRAE ?\"**  \n",
    "\n",
    "Vous devrez :   \n",
    "‚úÖ **T√©l√©charger et pr√©parer le mod√®le LLAMA 3**  \n",
    "‚úÖ **Cr√©er et pr√©-traiter un dataset sp√©cifique**  \n",
    "‚úÖ **Effectuer le fine-tuning avec LoRA**  \n",
    "‚úÖ **Sauvegarder le mod√®le entra√Æn√©**  \n",
    "‚úÖ **Convertir le mod√®le en format GGUF pour Ollama**  \n",
    "‚úÖ **Tester le mod√®le en lui posant la question cible**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import du mod√®le depuis hugging face\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [00:00<00:00, 40.43 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Traitement du jeu de donn√©es\n",
    "from datasets import load_dataset\n",
    "\n",
    "file_path = './data.jsonl'\n",
    "\n",
    "# ‚úÖ Charger le dataset JSONL\n",
    "dataset = load_dataset('json', data_files=file_path)\n",
    "\n",
    "\n",
    "# Pr√©traitement des donn√©es d'entrainement\n",
    "def preprocess_function(examples):\n",
    "    # üîπ Construire le texte d'entr√©e √† partir des messages\n",
    "    text_inputs = []\n",
    "    for message_set in examples[\"messages\"]:\n",
    "        text = \"\"\n",
    "        for message in message_set:\n",
    "            text += f\"{message['content']}\"\n",
    "        text_inputs.append(text.strip())\n",
    "\n",
    "    # üîπ Tokenisation\n",
    "    inputs = tokenizer(text_inputs, truncation=True, padding=\"max_length\", max_length=25)\n",
    "\n",
    "    # üîπ Copier input_ids pour labels\n",
    "    inputs[\"labels\"] = inputs[\"input_ids\"].copy()\n",
    "\n",
    "    # üîπ Remplacer les tokens de padding par -100 pour ignorer la perte\n",
    "    padding_token_id = tokenizer.pad_token_id\n",
    "    inputs[\"labels\"] = [\n",
    "        [(label if label != padding_token_id else -100) for label in labels] for labels in inputs[\"labels\"]\n",
    "    ]\n",
    "\n",
    "    return inputs\n",
    "\n",
    "# ‚úÖ Appliquer la transformation\n",
    "dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------\n",
      "trainable params: 6,815,744 || all params: 1,242,630,144 || trainable%: 0.5485\n",
      "---------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Quera\\Desktop\\INRAE-Llama3-main\\.llama_env\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "c:\\Users\\Quera\\Desktop\\INRAE-Llama3-main\\.llama_env\\Lib\\site-packages\\transformers\\training_args.py:1609: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ü§ó Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 02:05, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.148244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.028309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.844520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.644104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.434597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.194132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.943145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.699063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.450433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.103003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=2.3511348724365235, metrics={'train_runtime': 132.6604, 'train_samples_per_second': 0.302, 'train_steps_per_second': 0.075, 'total_flos': 7526107054080.0, 'train_loss': 2.3511348724365235, 'epoch': 10.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrainement du mod√®le\n",
    "\n",
    "n_epoches = 10\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "\n",
    "# ‚úÖ Config LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=32, lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "\n",
    "print(\"---------------------------------------------------------------\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "print(\"---------------------------------------------------------------\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama3-finetuned\",\n",
    "    per_device_train_batch_size=5,  # ‚ö†Ô∏è R√©duire la batch size car CPU est limit√©\n",
    "    num_train_epochs=n_epoches,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir='./logs',\n",
    "    learning_rate=2e-3,\n",
    "    gradient_accumulation_steps=90,  # üîπ Augmenter pour r√©duire la charge m√©moire\n",
    "    fp16=False,  # üö´ D√©sactiver fp16 (inutile sur CPU)\n",
    "    bf16=False,\n",
    "    gradient_checkpointing=False,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=100,\n",
    "    save_total_limit=2,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"tensorboard\",\n",
    "    torch_compile=False,  # ‚úÖ Optimisation CPU\n",
    "    no_cuda=True\n",
    ")\n",
    "\n",
    "\n",
    "# ‚úÖ Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "# ‚úÖ Entra√Ænement\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['train'],\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde du mod√®le\n",
    "# Sauvegarde du mod√®le LoRA et fusion des poids du mod√®le de base\n",
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "model.save_pretrained(\"llama3-finetuned\")\n",
    "tokenizer.save_pretrained(\"llama3-finetuned\")\n",
    "\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B-Instruct\", device_map=\"cpu\")\n",
    "peft_model = PeftModel.from_pretrained(base_model, \"llama3-finetuned\")\n",
    "\n",
    "\n",
    "# Fusionner les poids LoRA avec le mod√®le principal\n",
    "peft_model = peft_model.merge_and_unload()\n",
    "\n",
    "\n",
    "\n",
    "# Sauvegarde du mod√®le fusionn√© (sans LoRA)\n",
    "peft_model.save_pretrained(\"llama3-finetuned-merged\")\n",
    "tokenizer.save_pretrained(\"llama3-finetuned-merged\")\n",
    "\n",
    "print(\"‚úÖ Fusion et sauvegarde du mod√®le termin√© !\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertion du mod√®le avec CCP et Ollama\n",
    "!python ./llama.cpp/convert_hf_to_gguf.py ./llama3-finetuned-merged\n",
    "\n",
    "!ollama create llama-INRAE -f Modelfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Quera\\Desktop\\INRAE-Llama3-main\\fine-tuning.py\", line 22, in <module>\n",
      "    dataset = load_dataset('json', data_files=file_path)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Quera\\Desktop\\INRAE-Llama3-main\\.llama_env\\Lib\\site-packages\\datasets\\load.py\", line 2129, in load_dataset\n",
      "    builder_instance = load_dataset_builder(\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Quera\\Desktop\\INRAE-Llama3-main\\.llama_env\\Lib\\site-packages\\datasets\\load.py\", line 1849, in load_dataset_builder\n",
      "    dataset_module = dataset_module_factory(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Quera\\Desktop\\INRAE-Llama3-main\\.llama_env\\Lib\\site-packages\\datasets\\load.py\", line 1564, in dataset_module_factory\n",
      "    ).get_module()\n",
      "      ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Quera\\Desktop\\INRAE-Llama3-main\\.llama_env\\Lib\\site-packages\\datasets\\load.py\", line 944, in get_module\n",
      "    data_files = DataFilesDict.from_patterns(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Quera\\Desktop\\INRAE-Llama3-main\\.llama_env\\Lib\\site-packages\\datasets\\data_files.py\", line 721, in from_patterns\n",
      "    else DataFilesList.from_patterns(\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\Quera\\Desktop\\INRAE-Llama3-main\\.llama_env\\Lib\\site-packages\\datasets\\data_files.py\", line 624, in from_patterns\n",
      "    resolve_pattern(\n",
      "  File \"c:\\Users\\Quera\\Desktop\\INRAE-Llama3-main\\.llama_env\\Lib\\site-packages\\datasets\\data_files.py\", line 411, in resolve_pattern\n",
      "    raise FileNotFoundError(error_msg)\n",
      "FileNotFoundError: Unable to find 'C:/Users/Quera/Desktop/INRAE-Llama3-main\\'./data.jsonl''\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fine-tuning.py', 'meta-llama/Llama-3.2-1B-Instruct', \"'./data.jsonl'\", '10']\n"
     ]
    }
   ],
   "source": [
    "!python fine-tuning.py \"meta-llama/Llama-3.2-1B-Instruct\" './data.jsonl' 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.system(\"python ./llama.cpp/convert_hf_to_gguf.py ./llama3-finetuned-merged\")\n",
    "os.system(\"ollama create llama-INRAE -f Modelfile\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llama_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

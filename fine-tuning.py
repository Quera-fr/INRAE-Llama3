import sys
from datasets import load_dataset
from peft import LoraConfig, get_peft_model, PeftModel
from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq, AutoTokenizer, AutoModelForCausalLM


# Import du mod√®le depuis hugging face


model_name = "meta-llama/Llama-3.2-1B-Instruct"
file_path = './data.jsonl'
n_epoches = 20


tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

tokenizer.pad_token = tokenizer.eos_token

# ‚úÖ Charger le dataset JSONL
dataset = load_dataset('json', data_files=file_path)


# Pr√©traitement des donn√©es d'entrainement
def preprocess_function(examples):
    # üîπ Construire le texte d'entr√©e √† partir des messages
    text_inputs = []
    for message_set in examples["messages"]:
        text = ""
        for message in message_set:
            text += f"{message['content']}"
        text_inputs.append(text.strip())

    # üîπ Tokenisation
    inputs = tokenizer(text_inputs, truncation=True, padding="max_length", max_length=35)

    # üîπ Copier input_ids pour labels
    inputs["labels"] = inputs["input_ids"].copy()

    # üîπ Remplacer les tokens de padding par -100 pour ignorer la perte
    padding_token_id = tokenizer.pad_token_id
    inputs["labels"] = [
        [(label if label != padding_token_id else -100) for label in labels] for labels in inputs["labels"]
    ]

    return inputs

# ‚úÖ Appliquer la transformation
dataset = dataset.map(preprocess_function, batched=True)


# Entrainement du mod√®le
print("Entrainement du mod√®le :---------------------------------------------------------------")

# ‚úÖ Config LoRA
lora_config = LoraConfig(
    r=32, lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05, bias="none", task_type="CAUSAL_LM"
)


print("---------------------------------------------------------------")
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()
print("---------------------------------------------------------------")

training_args = TrainingArguments(
    output_dir="./llama3-finetuned",
    per_device_train_batch_size=5,  # ‚ö†Ô∏è R√©duire la batch size car CPU est limit√©
    num_train_epochs=n_epoches,
    save_strategy="epoch",
    evaluation_strategy="epoch",
    logging_dir='./logs',
    learning_rate=2e-3,
    gradient_accumulation_steps=90,  # üîπ Augmenter pour r√©duire la charge m√©moire
    fp16=False,  # üö´ D√©sactiver fp16 (inutile sur CPU)
    bf16=False,
    gradient_checkpointing=False,
    lr_scheduler_type="cosine",
    warmup_steps=100,
    save_total_limit=2,
    weight_decay=0.01,
    report_to="tensorboard",
    torch_compile=False,  # ‚úÖ Optimisation CPU
    no_cuda=True
)


# ‚úÖ Data collator
data_collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8, return_tensors="pt")


# ‚úÖ Entra√Ænement
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset['train'],
    eval_dataset=dataset['train'],
    data_collator=data_collator
)

trainer.train()


print("‚úÖ Entrainement du mod√®le termin√© !")

model.save_pretrained("llama3-finetuned")
tokenizer.save_pretrained("llama3-finetuned")


base_model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.2-1B-Instruct", device_map="cpu")
peft_model = PeftModel.from_pretrained(base_model, "llama3-finetuned")


# Fusionner les poids LoRA avec le mod√®le principal
peft_model = peft_model.merge_and_unload()

# Sauvegarde du mod√®le fusionn√© (sans LoRA)
peft_model.save_pretrained("llama3-finetuned-merged")
tokenizer.save_pretrained("llama3-finetuned-merged")

print("‚úÖ Fusion et sauvegarde du mod√®le termin√© !")

import os

os.system("python ./llama.cpp/convert_hf_to_gguf.py ./llama3-finetuned-merged")
os.system("ollama create llama-INRAE -f Modelfile")